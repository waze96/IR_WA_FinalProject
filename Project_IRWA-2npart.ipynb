{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHK-yP0fTAP9"
   },
   "source": [
    "# Information Retrieval and Web Analytics: Indexing + Modeling (TF-IDF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 617,
     "status": "ok",
     "timestamp": 1603470874764,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -120
    },
    "id": "9NUq8PwCTAQA",
    "outputId": "8fa7a9c5-8127-4cae-acb1-44239837794b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/waze/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# if you do not have nltk the following command should work \"python -m pip install nltk\" \n",
    "import nltk\n",
    "nltk.download('stopwords');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 615,
     "status": "ok",
     "timestamp": 1603473061105,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -120
    },
    "id": "8F9PVJlbTAQF",
    "outputId": "1ea5d092-3739-495f-a112-4b492561eb71"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "data_path = './data/one2.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlNg4TFXTAQI"
   },
   "source": [
    "#### Load data into memory\n",
    "\n",
    "As mentioned above the dataset is stored in a tsv file ```parsed_input_500.tsv```and it contains 500 Wikipedia article (one article per line). For each article we have id, title and body separated by \"|\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2796,
     "status": "ok",
     "timestamp": 1603473064968,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -120
    },
    "id": "X4P3KzRMTAQJ"
   },
   "outputs": [],
   "source": [
    "docs_path = data_path\n",
    "with open(docs_path) as fp:\n",
    "    lines = fp.readlines()\n",
    "tweets = [l.strip().replace(' +', ' ') for l in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 557,
     "status": "ok",
     "timestamp": 1603473068069,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -120
    },
    "id": "BiGNBAAeTAQM",
    "outputId": "4444d85b-c071-44f1-c385-454b4bf5207d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total numer of tweets in the corpus: 1001\n"
     ]
    }
   ],
   "source": [
    "print(\"Total numer of tweets in the corpus: {}\" .format(len(tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweet(tweetText):\n",
    "    \"\"\"\n",
    "    Preprocess the article text (title + body) removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    line -- string (text) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    line - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "        \n",
    "    stemming = PorterStemmer()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    ## START CODE\n",
    "    line= tweetText.lower() ## Transform in lowercase\n",
    "    line=re.sub('[:\\[\\]&%$\\\"\\'!./,;:?=¿^\\-#_*+)<>(¡@]','',line)\n",
    "    line= line.split() ## Tokenize the text to get a list of terms\n",
    "    line= [word for word in line if word not in stops]  ##eliminate the stopwords (HINT: use List Comprehension)\n",
    "    line= [stemming.stem(word) for word in line] ## perform stemming (HINT: use List Comprehension)\n",
    "    ## END CODE\n",
    "    return line\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def getRelevantInfo(tweet):\n",
    "    dictRelevantInfo ={}\n",
    "    data = json.loads(tweet)\n",
    "    hashtags = []\n",
    "    urlsList = []\n",
    "    text = ''\n",
    "    date = data['created_at'] ## ??? RT o no RT\n",
    "    try:\n",
    "        isRt=True\n",
    "        isRetweet=data[\"retweeted_status\"]\n",
    "        idTweet=isRetweet[\"id_str\"]\n",
    "        text = isRetweet['text']\n",
    "        username = isRetweet['user']['screen_name']\n",
    "        urls = isRetweet['entities']['urls']\n",
    "        rt_count = isRetweet['retweet_count']\n",
    "        likes = isRetweet['favorite_count']\n",
    "        \n",
    "        for h in isRetweet['entities']['hashtags']:\n",
    "            hashtags.append(h['text'])\n",
    "        for url in urls:\n",
    "            urlsList.append(url['url'])\n",
    "            \n",
    "    except:\n",
    "        isRt=False\n",
    "        idTweet=data[\"id_str\"]\n",
    "        text = data['text']\n",
    "        username = data['user']['screen_name']\n",
    "        urls = data['entities']['urls']\n",
    "        rt_count=data['retweet_count']\n",
    "        likes = data['favorite_count']\n",
    "        \n",
    "        for h in data['entities']['hashtags']:\n",
    "            hashtags.append(h['text'])\n",
    "            \n",
    "        for url in urls:\n",
    "            urlsList.append(url['url'])        \n",
    "            \n",
    "    dictRelevantInfo['tweetID'] = idTweet\n",
    "    dictRelevantInfo['text'] = text\n",
    "    dictRelevantInfo['tokens'] = cleanTweet(text)\n",
    "    dictRelevantInfo['username'] = username\n",
    "    dictRelevantInfo['date'] = date\n",
    "    dictRelevantInfo['hashtags'] = hashtags\n",
    "    dictRelevantInfo['likes'] = likes\n",
    "    dictRelevantInfo['rt_count'] = rt_count\n",
    "    dictRelevantInfo['urlsList'] = urlsList\n",
    "    dictRelevantInfo['isRetweeted'] = isRt\n",
    "    return dictRelevantInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanTweets = {}\n",
    "for t in tweets:\n",
    "    currentTweet=getRelevantInfo(t)\n",
    "    tweetID=currentTweet['tweetID']\n",
    "    isRtCurrent=currentTweet['isRetweeted']\n",
    "    # Orignial tweet found, overwrite if retweet exist.\n",
    "    if isRtCurrent == False:\n",
    "        cleanTweets[tweetID] = currentTweet\n",
    "    else:\n",
    "        if tweetID in cleanTweets:\n",
    "            continue\n",
    "        else:\n",
    "            cleanTweets[tweetID] = currentTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of cleaned tweets:  709\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of cleaned tweets: \",len(cleanTweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 579,
     "status": "ok",
     "timestamp": 1603473436846,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -120
    },
    "id": "j2uIObKMTAQS"
   },
   "outputs": [],
   "source": [
    "def create_index(lines):\n",
    "    \"\"\"\n",
    "    Impleent the inverted index\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a python dictionary) containing terms as keys and the corresponding \n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    \"\"\"\n",
    "    index=defaultdict(list) \n",
    "    tweetIndex = {} # dictionary to map tweets to page ids\n",
    "    for line in lines.values(): # Remember, lines contain all tweets, each line is a tweet\n",
    "        tweetID = line['tweetID']\n",
    "        terms = line['tokens'] #page_title + page_text\n",
    "        tweetIndex[tweetID]=line['text']  ## we do not need to apply get terms to title because it used only to print titles and not in the index\n",
    "        \n",
    "        ## ===============================================================        \n",
    "        ## create the index for the current doc and store it in termdictPage\n",
    "        ## termdictPage ==> { ‘term1’: [currentdoc, [list of positions]], ...,‘termn’: [currentdoc, [list of positions]]}\n",
    "        \n",
    "        ## Example: if the curr_doc has id 1 and his text is \n",
    "        ## \"web retrieval information retrieval\":\n",
    "        \n",
    "        ## termdictPage ==> { ‘web’: [1, [0]], ‘retrieval’: [1, [1,4]], ‘information’: [1, [2]]}\n",
    "        \n",
    "        ## the term ‘web’ appears in document 1 in positions 0, \n",
    "        ## the term ‘retrieval’ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "        \n",
    "        termdictPage={}\n",
    "\n",
    "        for position, term in enumerate(terms): # terms contains page_title + page_text. Loop over all terms\n",
    "            try:\n",
    "                # if the term is already in the index for the current page (termdictPage)\n",
    "                # append the position to the corrisponding list\n",
    "                \n",
    "        ## START CODE\n",
    "                termdictPage[term][1].append(position)  \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                termdictPage[term]=[tweetID, array('I',[position])] #'I' indicates unsigned int (int in python)\n",
    "            \n",
    "        #merge the current page index with the main index\n",
    "        for termpage, postingpage in termdictPage.items():\n",
    "            index[termpage].append(postingpage)\n",
    "        \n",
    "        ## END CODE                    \n",
    "                    \n",
    "    return index, tweetIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 7462,
     "status": "ok",
     "timestamp": 1603475667598,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -120
    },
    "id": "L388FPECTAQV",
    "outputId": "2669a9b6-d767-4b78-f617-130ba69488c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 0.01 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# index is the dict of , index by twee ?¿?¿\n",
    "# tweetIndex is the dict of tweets, indexed by tweetID\n",
    "index, tweetIndex = create_index(cleanTweets)\n",
    "\n",
    "print(\"Total time to create the index: {} seconds\" .format(np.round(time.time() - start_time,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyd2WyJOTAQY"
   },
   "source": [
    "Notice that if you look in the index for ```researcher```you will not find any result, while if you look for ```research``` you will get some results. That happens because we are storing in the index stemmed terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "executionInfo": {
     "elapsed": 846,
     "status": "ok",
     "timestamp": 1603473583036,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -120
    },
    "id": "mZuV9k_ATAQZ",
    "outputId": "93d2bb77-8ca6-4d9d-acab-b5896f5aad53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Index results for the term 'biden': 8\n",
      "\n",
      "First 10 Index results for the term 'biden': \n",
      "[['1331952640574459905', array('I', [1])], ['1333092424936284163', array('I', [3])], ['1333092425611554823', array('I', [8])], ['1333088877343608832', array('I', [6])], ['1333026911799439361', array('I', [7])], ['1332712274000293889', array('I', [3])], ['1333092426953658368', array('I', [7])], ['1333023981646057473', array('I', [6])], ['1333087495509012480', array('I', [0])], ['1333088498975436804', array('I', [2])]]\n"
     ]
    }
   ],
   "source": [
    "#print(\"Index results for the term 'biden': {}\\n\".format(index['biden']))\n",
    "print(\"Length of Index results for the term 'biden': {}\\n\".format(len(index['obama'])))\n",
    "print(\"First 10 Index results for the term 'biden': \\n{}\".format(index['biden'][:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResultsFromIndex(word, index):\n",
    "    stemming = PorterStemmer()\n",
    "    newWord=word.lower() ## Transform in lowercase\n",
    "    newWord=re.sub('[:\\[\\]&%$\\\"\\'!./,;:?=¿^\\-#_*+)<>(¡@]','',word)\n",
    "    newWord= stemming.stem(word) ## perform stemming\n",
    "    print(\"Length of Index results for the term '{}': {}\".format(newWord, len(index[newWord])))\n",
    "    print(\"First 10 Index results for the term '{}': \\n{}\\n\\n\".format(newWord, index[newWord][:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Index results for the term 'trump': 326\n",
      "First 10 Index results for the term 'trump': \n",
      "[['1332994175281868801', array('I', [6])], ['1333068258862370819', array('I', [0])], ['1333068652665516036', array('I', [3])], ['1332927520270913536', array('I', [8])], ['1333057200802107393', array('I', [9])], ['1333091717579501569', array('I', [2])], ['1333071374081011715', array('I', [5])], ['1333023981646057473', array('I', [3])], ['1333070773758685194', array('I', [0])], ['1332844785334358016', array('I', [1])]]\n",
      "\n",
      "\n",
      "Length of Index results for the term 'biden': 155\n",
      "First 10 Index results for the term 'biden': \n",
      "[['1331952640574459905', array('I', [1])], ['1333092424936284163', array('I', [3])], ['1333092425611554823', array('I', [8])], ['1333088877343608832', array('I', [6])], ['1333026911799439361', array('I', [7])], ['1332712274000293889', array('I', [3])], ['1333092426953658368', array('I', [7])], ['1333023981646057473', array('I', [6])], ['1333087495509012480', array('I', [0])], ['1333088498975436804', array('I', [2])]]\n",
      "\n",
      "\n",
      "Length of Index results for the term 'joe': 50\n",
      "First 10 Index results for the term 'joe': \n",
      "[['1333088877343608832', array('I', [5])], ['1333026911799439361', array('I', [6])], ['1332712274000293889', array('I', [2])], ['1333088498975436804', array('I', [1])], ['1333089405523824644', array('I', [5])], ['1333025108458430464', array('I', [0])], ['1332391784278798338', array('I', [11])], ['1324907501498855425', array('I', [2])], ['1332816778578169861', array('I', [5])], ['1333034437647413250', array('I', [6])]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "getResultsFromIndex(\"trump\", index)\n",
    "getResultsFromIndex(\"biden\", index)\n",
    "getResultsFromIndex(\"joe\", index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atoTwD8uTAQc"
   },
   "source": [
    "### Querying the Index\n",
    "\n",
    "Even if before we mentioned that in case of phrase queries we need to take into account the position of the terms in the document and we have implemented an index that would allow us to also work with this type of queries, here you are going to implement a search function that will query the index without take into account the trems' positions.\n",
    "\n",
    "\n",
    "We will use english Free Text Queries, that means that the query we will query the index using  a sequence of english words as query, and the output will be the list of documents that contain any of the query terms. \n",
    "\n",
    "For instance if we write the query **\"computer science\"** the output will be the union of all documents containing the term \"computer\" with all documents containing the term \"science\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 610,
     "status": "ok",
     "timestamp": 1603474126566,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -120
    },
    "id": "HgJ-tjraTAQc"
   },
   "outputs": [],
   "source": [
    "def search(query, index):\n",
    "    '''\n",
    "    The output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    '''\n",
    "    query=cleanTweet(query)\n",
    "    tweets=set()\n",
    "    for term in query:\n",
    "    ## START DODE\n",
    "        try:\n",
    "            # store in termTweets the ids of the tweets that contain \"term\"                        \n",
    "            termTweets=[posting[0] for posting in index[term]]\n",
    "            # tweets = tweets Union termTweets\n",
    "            tweets = tweets.union(termTweets)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    tweets=list(tweets)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "executionInfo": {
     "elapsed": 9085,
     "status": "ok",
     "timestamp": 1603474152854,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -120
    },
    "id": "TZ9WpXCLTAQf",
    "outputId": "62304f17-8569-491c-f185-a168289814df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:\n",
      "\n",
      "biden\n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 155 for the seached query:\n",
      "\n",
      "tweet_id= 1333090140168839168\n",
      "tweet_text: Biden won again\n",
      "\n",
      "\n",
      "\n",
      "tweet_id= 1333092468363935745\n",
      "tweet_text: @JoeBiden Thank you for your leadership President Biden!\n",
      "\n",
      "\n",
      "\n",
      "tweet_id= 1332842624349159424\n",
      "tweet_text: @tedcruz Three of the last four Republican Presidents have lowered the top marginal income tax rate. Biden would be… https://t.co/1vCnPCnP4n\n",
      "\n",
      "\n",
      "\n",
      "tweet_id= 1333010636201201665\n",
      "tweet_text: If his 80 million 'votes' were legit, Biden wouldn't need to pay click farms for 'likes'.\n",
      "\n",
      "\n",
      "\n",
      "tweet_id= 1333086200652058626\n",
      "tweet_text: On Day One, Biden can cancel federal student debt, strengthen labor unions and boost overtime pay.\n",
      "\n",
      "His top priorit… https://t.co/92nF07WpOC\n",
      "\n",
      "\n",
      "\n",
      "tweet_id= 1332712274000293889\n",
      "tweet_text: I just have to reiterate the fact that Joe Biden got destroyed three times prior to 2020 and had to be dragged acro… https://t.co/lFak0GOAhM\n",
      "\n",
      "\n",
      "\n",
      "tweet_id= 1333089772907192322\n",
      "tweet_text: BREAKING: The recount in Wisconsin is completed and Joe Biden wins again. This time by additional 87 votes.\n",
      "\n",
      "Trump… https://t.co/YUITuptiQT\n",
      "\n",
      "\n",
      "\n",
      "tweet_id= 1332714695632048130\n",
      "tweet_text: President-elect Biden won by a landslide and in 1,273 hours he will be sworn in.\n",
      "\n",
      "\n",
      "\n",
      "tweet_id= 1333092462546595842\n",
      "tweet_text: 3 million well spent\n",
      "\n",
      "that's like uhhh\n",
      "\n",
      "260 vote biden got more lmao\n",
      "\n",
      "\n",
      "\n",
      "tweet_id= 1333092452291448840\n",
      "tweet_text: @joncoopertweets Wanna bet Biden NEVER gets sworn in?\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query:\\n\")\n",
    "query = input()\n",
    "docs = search(query, index)    \n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the seached query:\\n\".format(top, len(docs)))\n",
    "for d_id in docs[:top] :\n",
    "    print(\"tweet_id= {}\\ntweet_text: {}\\n\\n\\n\".format(d_id, tweetIndex[d_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMUIrY9bTAQi"
   },
   "source": [
    "### Ranking tf-idf\n",
    "\n",
    "When searching in a search engine, we are interested in obtain the results sorted by relevance or by some other criteria. Notice that **the above results are not ranked**.\n",
    "\n",
    "Here you are going to implement **tf-idf (Term Frequency — Inverse Document Frequency)** and use it to obtain a list of ordered results.\n",
    "\n",
    "Tf-idf is a weighting scheme that assigns each term in a document a weight based on its term frequency (tf) and inverse document frequency (idf).  The higher the scores, more important the term is. \n",
    "\n",
    "##### TF\n",
    "**tf** refers to the frequency of a term $t$ in a specific document $d$. The basic idea is that as a term appears more in the document it becomes more important. On the other side, if we only use pure term counts, longer documents will be favored more. Consider two documents with exactly the same content but one being twice longer by concatenating with itself.  The tf weights of each word in the longer document will be twice the shorter one, although they essentially have the same content. To deal with this issue we need to **normalize the term frequencies**.\n",
    "\n",
    "$$tf_{t,d} = \\dfrac{N_{t,d}}{||D||}\\tag{1}$$\n",
    "\n",
    "\n",
    "\n",
    "where ||D|| is the Euclidean norm. \n",
    "\n",
    "\n",
    "Let $D=[t_1, t_2, \\dots, t_n]$ be the document vector where $t_i$ represent the frequency of the term $i$, the  Euclidean Norm is calculated as\n",
    "\n",
    "$$\\sqrt{\\sum_{t=1}^{n}t_i{^2}}\\tag{2}$$\n",
    "\n",
    "Note that $||D||$ is the same for all terms of a document.\n",
    "\n",
    "\n",
    "##### IDF\n",
    "A drawback of tf is that it considers all terms equally important. However, less common terms are more discriminative than others. To deal with this issue we introduce **idf (inverse document frequency)** that takes into account the number of documents containing the term.\n",
    "\n",
    "$$idf_t = log\\dfrac{N}{df_t}\\tag{3}$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $N$ is the total number of documents;\n",
    "- $df_t$ is the number of ocuments containing the term $t$.\n",
    "\n",
    "The log operation is applied to avoid that terms that appears in a high number of documents are considered to be too much less important, in this way we are smoothing (dampening) this difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 560,
     "status": "ok",
     "timestamp": 1603477067467,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -120
    },
    "id": "FMjGU0ZbTAQj"
   },
   "outputs": [],
   "source": [
    "def create_index_tfidf(lines, numDocuments):\n",
    "    \"\"\"\n",
    "    Implement the inverted index and compute tf, df and idf\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    numDocuments -- total number of documents\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a python dictionary) containing terms as keys and the corresponding \n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    tf - normalized term frequency for each term in each document\n",
    "    df - number of documents each term appear in\n",
    "    idf - inverse document frequency of each term\n",
    "    \"\"\"\n",
    "        \n",
    "    index=defaultdict(list)\n",
    "    tf=defaultdict(list) #term frequencies of terms in documents (documents in the same order as in the main index)\n",
    "    df=defaultdict(int)         #document frequencies of terms in the corpus\n",
    "    titleIndex=defaultdict(str)\n",
    "    idf=defaultdict(float)\n",
    "    \n",
    "    for line in lines.values(): # Remember, lines contain all tweets, each line is a tweet\n",
    "        #tweetID = line['tweetID']\n",
    "        tweetID = line['tweetID']        \n",
    "        terms = line['tokens']\n",
    "        tweetIndex[tweetID]=line['text']           \n",
    "        \n",
    "        ## ===============================================================        \n",
    "        ## create the index for the **current page** and store it in termdictPage\n",
    "        ## termdictPage ==> { ‘term1’: [currentdoc, [list of positions]], ...,‘termn’: [currentdoc, [list of positions]]}\n",
    "        \n",
    "        ## Example: if the curr_doc has id 1 and his text is \n",
    "        ## \"web retrieval information retrieval\":\n",
    "        \n",
    "        ## termdictPage ==> { ‘web’: [1, [0]], ‘retrieval’: [1, [1,4]], ‘information’: [1, [2]]}\n",
    "        \n",
    "        ## the term ‘web’ appears in document 1 in positions 0, \n",
    "        ## the term ‘retrieval’ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "        termdictPage={}\n",
    "\n",
    "        for position, term in enumerate(terms): ## terms contains page_title + page_text\n",
    "            try:\n",
    "                # if the term is already in the dict append the position to the corrisponding list\n",
    "                termdictPage[term][tweetID].append(position) \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                termdictPage[term]=[tweetID, array('I',[position])] #'I' indicates unsigned int (int in python)\n",
    "        \n",
    "        #normalize term frequencies\n",
    "        # Compute the denominator to normalize term frequencies (formula 2 above)\n",
    "        # norm is the same for all terms of a document.\n",
    "        norm=0\n",
    "        for term, posting in termdictPage.items(): \n",
    "            # posting is a list containing doc_id and the list of positions for current term in current document: \n",
    "            # posting ==> [currentdoc, [list of positions]] \n",
    "            # you can use it to inferr the frequency of current term.\n",
    "            norm+=len(posting[1])**2\n",
    "        norm=math.sqrt(norm)\n",
    "\n",
    "\n",
    "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in termdictPage.items():     \n",
    "            # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "            tf[term].append(np.round(len(posting[1])/norm,4))  ## SEE formula (1) above\n",
    "            #increment the document frequency of current term (number of documents containing the current term)\n",
    "            df[term]= len(termdictPage[term])  # increment df for current term\n",
    "        \n",
    "        #merge the current page index with the main index\n",
    "        for termpage, postingpage in termdictPage.items():\n",
    "            index[termpage].append(postingpage)\n",
    "            \n",
    "        # Compute idf following the formula (3) above. HINT: use np.log\n",
    "        for term in df:\n",
    "            idf[term] = np.round(np.log(float(numDocuments/df[term])),4)\n",
    "            \n",
    "    return index, tf, df, idf, tweetIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLikeRTIndex(lines, numDocuments):\n",
    "    maxLikes = 0\n",
    "    maxRT = 1\n",
    "    for tweet in lines:\n",
    "        if lines[tweet]['likes'] > maxLikes:\n",
    "            maxLikes = lines[tweet]['likes']\n",
    "        if lines[tweet]['rt_count'] > maxRT:\n",
    "            maxRT = lines[tweet]['rt_count']\n",
    "    for tweet in lines:\n",
    "        lines[tweet]['likes_score'] = (-np.exp(-(lines[tweet]['likes']/50000))+1)\n",
    "        lines[tweet]['rt_score'] = (-np.exp(-(lines[tweet]['rt_count']/25000))+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 84674,
     "status": "ok",
     "timestamp": 1603477153237,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -120
    },
    "id": "Mvl-m4H-TAQl",
    "outputId": "11c2673f-463b-42e0-9826-9c43a9af576b"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-1bb9aafc5e4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnumTweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleanTweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweetIndex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_index_tfidf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleanTweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumTweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total time to create the index: {} seconds\"\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-9a703712fe0f>\u001b[0m in \u001b[0;36mcreate_index_tfidf\u001b[0;34m(lines, numDocuments)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# Compute idf following the formula (3) above. HINT: use np.log\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0midf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumDocuments\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweetIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mround_\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mround_\u001b[0;34m(a, decimals, out)\u001b[0m\n\u001b[1;32m   3626\u001b[0m \u001b[0;31m# they can have unique docstrings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3628\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0marray_function_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_around_dispatcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3629\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mround_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3630\u001b[0m     \"\"\"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "numTweets = len(cleanTweets)\n",
    "index, tf, df, idf, tweetIndex = create_index_tfidf(cleanTweets, numTweets)\n",
    "print(\"Total time to create the index: {} seconds\" .format(np.round(time.time() - start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57724\n",
      "0.6847785905742336\n"
     ]
    }
   ],
   "source": [
    "createLikeRTIndex(cleanTweets, numTweets)\n",
    "\n",
    "print(cleanTweets['1323630540977852416']['likes'])\n",
    "print(cleanTweets['1323630540977852416']['likes_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDictToCSV(d, filename):\n",
    "    w = csv.writer(open(filename, \"w\"))\n",
    "    for key, val in d.items():\n",
    "        w.writerow([key, val])\n",
    "        \n",
    "def loadDictFromCSV(filename):\n",
    "    tmpDict={}\n",
    "    with open(filename,'r') as data: \n",
    "        for line in csv.reader(data): \n",
    "            tmpDict[line[0]]=line[1]\n",
    "    return tmpDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "executionInfo": {
     "elapsed": 82645,
     "status": "ok",
     "timestamp": 1603477153238,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -120
    },
    "id": "2pir0XHJTAQp"
   },
   "outputs": [],
   "source": [
    "def rankDocuments(terms, docs, index, idf, tf, titleIndex):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    titleIndex -- mapping between page id and page title\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "        \n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    # The remaing elements would became 0 when multiplied to the queryVector\n",
    "    docVectors=defaultdict(lambda: [0]*len(terms)) # I call docVectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary \n",
    "    queryVector=[0]*len(terms)    \n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms) # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
    "    # HINT: use when computing tf for queryVector\n",
    "    \n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "    \n",
    "    for termIndex, term in enumerate(terms): #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "                    \n",
    "        ## Compute tf*idf(normalize tf as done with documents)\n",
    "        queryVector[termIndex]=query_terms_count[term]/query_norm * idf[term]\n",
    "\n",
    "        # Generate docVectors for matching docs\n",
    "        for docIndex, (doc, postings) in enumerate(index[term]):\n",
    "            # Example of [docIndex, (doc, postings)]\n",
    "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "            \n",
    "            #tf[term][0] will contain the tf of the term \"term\" in the doc 26            \n",
    "            if doc in docs:\n",
    "                docVectors[doc][termIndex]=tf[term][docIndex] * idf[term]  # TODO: check if multiply for idf\n",
    "\n",
    "    # calculate the score of each tweet\n",
    "    # compute the cosine similarity between queyVector and each docVector:\n",
    "    # HINT: you can use the dot product because in case of normalized vectors it corresponds to the cosine siilarity\n",
    "    # see np.dot\n",
    "    \n",
    "    docScores=[ [(np.dot(curDocVec, queryVector)/(np.linalg.norm(curDocVec)*np.linalg.norm(queryVector)))*0.6+cleanTweets[doc]['likes_score']*0.2+cleanTweets[doc]['rt_score']*0.2, doc] for doc, curDocVec in docVectors.items() ]\n",
    "    docScores.sort(reverse=True)\n",
    "    resultDocs=[(x[0], x[1]) for x in docScores]\n",
    "    #print document titles instead if document id's\n",
    "    #resultDocs=[ titleIndex[x] for x in resultDocs ]\n",
    "    if len(resultDocs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)    \n",
    "    #print ('\\n'.join(resultDocs), '\\n')\n",
    "    return resultDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "executionInfo": {
     "elapsed": 80771,
     "status": "ok",
     "timestamp": 1603477153238,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -120
    },
    "id": "CLdEEhT1TAQr"
   },
   "outputs": [],
   "source": [
    "def search_tf_idf(query, index):\n",
    "    '''\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    '''\n",
    "    query=cleanTweet(query)\n",
    "    docs=set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in termDocs the ids of the docs that contain \"term\"                        \n",
    "            termDocs=[posting[0] for posting in index[term]]\n",
    "            \n",
    "            # docs = docs Union termDocs\n",
    "            docs = docs.union(termDocs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs=list(docs)\n",
    "    ranked_docs = rankDocuments(query, docs, index, idf, tf, tweetIndex)   \n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "EpTgX-pTTAQu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:\n",
      "\n",
      "Impossible biden\n",
      "\n",
      "======================\n",
      "Top 10 results out of 155 for the seached query:\n",
      "\n",
      "Tweet_id= 1332155514386599937\n",
      "Tweet: A must read. Impossible for Biden to have overcome these, and even greater, odds! https://t.co/cmYFY0va6p\n",
      "Likes: 191740\n",
      "RT: 53268\n",
      "Score: 0.9267581131509772\n",
      "\n",
      "\n",
      "Tweet_id= 1332407714304110597\n",
      "Tweet: Biden did poorly in big cities (Politico), except those of Detroit (more votes than people!), Philadelphia, Atlanta… https://t.co/0cCyv36E31\n",
      "Likes: 255510\n",
      "RT: 56691\n",
      "Score: 0.7586967688864448\n",
      "\n",
      "\n",
      "Tweet_id= 930065838387863552\n",
      "Tweet: It's time to talk about former Vice President Joe Biden, the open sexual predator. A thread/moment...\n",
      "Likes: 65597\n",
      "RT: 49910\n",
      "Score: 0.6966962020354317\n",
      "\n",
      "\n",
      "Tweet_id= 1332527601798275073\n",
      "Tweet: Trump really blew $3,000,000 to give Biden 132 more votes, that has to be the L of the decade.\n",
      "Likes: 234811\n",
      "RT: 19772\n",
      "Score: 0.6877613872031622\n",
      "\n",
      "\n",
      "Tweet_id= 1323630540977852416\n",
      "Tweet: Don't be surprised if you hear a network call the election for Biden before the polls are closed today.  \n",
      "\n",
      "The disi… https://t.co/Lr2lY3hXQY\n",
      "Likes: 57724\n",
      "RT: 19762\n",
      "Score: 0.626516111738799\n",
      "\n",
      "\n",
      "Tweet_id= 1332721647741243392\n",
      "Tweet: Actually, yes.\n",
      "In 2012, Obama ran against a good, decent man. They differed on policy.\n",
      "Joe Biden ran against the hu… https://t.co/yKxlgiPnsh\n",
      "Likes: 83343\n",
      "RT: 9319\n",
      "Score: 0.6205046204376629\n",
      "\n",
      "\n",
      "Tweet_id= 1332734431107354629\n",
      "Tweet: Biden “won” in the same way that Lance Armstrong “won.” Yes, Armstrong crossed the finish line first. But his victo… https://t.co/HpKu6a5d2Z\n",
      "Likes: 63038\n",
      "RT: 13609\n",
      "Score: 0.6152329345114677\n",
      "\n",
      "\n",
      "Tweet_id= 1332400909536653312\n",
      "Tweet: Raise your hand if you're one of the 80,085,824 people who voted for Joe Biden 👋\n",
      "Likes: 87853\n",
      "RT: 3356\n",
      "Score: 0.6027371877836998\n",
      "\n",
      "\n",
      "Tweet_id= 1332854101881856000\n",
      "Tweet: Again, we don’t know who needs to hear this, but it’s quite clear that Biden cheated.\n",
      "Likes: 39463\n",
      "RT: 7561\n",
      "Score: 0.5614962051726253\n",
      "\n",
      "\n",
      "Tweet_id= 1332714695632048130\n",
      "Tweet: President-elect Biden won by a landslide and in 1,273 hours he will be sworn in.\n",
      "Likes: 32447\n",
      "RT: 2553\n",
      "Score: 0.5296998498727249\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query:\\n\")\n",
    "query = input()\n",
    "ranked_docs = search_tf_idf(query, index)    \n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the seached query:\\n\".format(top, len(ranked_docs)))\n",
    "for d_score, d_id in ranked_docs[:top] :\n",
    "    print(\"Tweet_id= {}\\nTweet: {}\\nLikes: {}\\nRT: {}\\nScore: {}\\n\\n\".format(d_id, tweetIndex[d_id], cleanTweets[d_id]['likes'], cleanTweets[d_id]['rt_count'], d_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FLR89baTAQw"
   },
   "outputs": [],
   "source": [
    "# LOCURA (-e^-(x/50000)+1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Model_Tf_IDF_Index_stud.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
