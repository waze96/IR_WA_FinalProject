{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHK-yP0fTAP9"
   },
   "source": [
    "# Information Retrieval and Web Analytics: Indexing + Modeling (TF-IDF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 882,
     "status": "ok",
     "timestamp": 1607423766966,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -60
    },
    "id": "8F9PVJlbTAQF",
    "outputId": "d54264e2-cd8a-4860-e2be-e39b9e0047e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/waze/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# ## Uncoment if using google colab\\nfrom google.colab import drive\\nfrom google.colab.patches import cv2_imshow\\ndrive.mount('/content/drive') \\ndata_path = '/content/drive/My Drive/Information retrieval/IR_WA_FinalProject-master/data/'\\n\\nfile_path = data_path + 'one100K_v2.json\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "nltk.download('stopwords');\n",
    "\n",
    "file_path = '../data/one100K_v2.json'\n",
    "\n",
    "'''\n",
    "# ## Uncoment if using google colab\n",
    "from google.colab import drive\n",
    "from google.colab.patches import cv2_imshow\n",
    "drive.mount('/content/drive') \n",
    "data_path = '/content/drive/My Drive/Information retrieval/IR_WA_FinalProject-master/data/'\n",
    "\n",
    "file_path = data_path + 'one100K_v2.json'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PDgndzJeurq"
   },
   "source": [
    "## **Preprocessing Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlNg4TFXTAQI"
   },
   "source": [
    "#### Load data into memory\n",
    "\n",
    "The dataset ```dataset.json``` contains a list of N tweets and its information. This dataset has ben made scrapping for tweets that contain any of the words \"Trump\", \"#Trump\", \"Biden\", \"#Biden\", \"#UsElections2020\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 8951,
     "status": "ok",
     "timestamp": 1607423776347,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -60
    },
    "id": "X4P3KzRMTAQJ"
   },
   "outputs": [],
   "source": [
    "docs_path = file_path\n",
    "with open(docs_path) as fp:\n",
    "    lines = fp.readlines()\n",
    "tweets = [l.strip().replace(' +', ' ') for l in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6586,
     "status": "ok",
     "timestamp": 1607419247053,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -60
    },
    "id": "BiGNBAAeTAQM",
    "outputId": "f8ce9110-1bc4-46fc-a64c-5aff48192db7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total numer of tweets in the corpus: 100001\n"
     ]
    }
   ],
   "source": [
    "print(\"Total numer of tweets in the corpus: {}\" .format(len(tweets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_A3K2wbqUJ9B"
   },
   "source": [
    "#### Clean Tweets\n",
    "Preprocess the text of a concrete tweet removing non alphabetic characters, stop words, stemming, transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:  tweetText -- string (text) to be preprocessed    \n",
    "    Returns:   cleanText - a list of tokens corresponding to the tweetText after the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 1089,
     "status": "ok",
     "timestamp": 1607419254114,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -60
    },
    "id": "6Bw4AjprSJHp"
   },
   "outputs": [],
   "source": [
    "def cleanTweet(tweetText):      \n",
    "    stemming = PorterStemmer()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "    cleanText = tweetText.lower() ## Transform in lowercase\n",
    "    cleanText = re.sub('[:\\[\\]&%$\\\"\\'!./,;:?=¿^\\-_*+)<>(¡]','',cleanText)\n",
    "    cleanText = cleanText.split() ## Tokenize the text to get a list of terms\n",
    "    cleanText = [word for word in cleanText if word not in stops]  ##eliminate the stopwords\n",
    "    cleanText = [stemming.stem(word) for word in cleanText] ## perform stemming\n",
    "    for i in range(len(cleanText)):\n",
    "        if cleanText[i][0] == '@':\n",
    "            cleanText[i] = str(hash(cleanText[i]))\n",
    "    return cleanText\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryhRZ8N-VTHI"
   },
   "source": [
    "#### Crate a dictionary for each tweet\n",
    "For each tweet, creates a dictionary containing the most relevant information of it (Username, OriginalText, Clean Tokens, number of Likes, number of retweets, list of URLs...)\n",
    "    \n",
    "    Argument:  tweet -- a JSON tweet content    \n",
    "    Returns:   dictRelevantInfo -- a dictionary with the processed tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 1007,
     "status": "ok",
     "timestamp": 1607419256787,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -60
    },
    "id": "_dUAcWTaSJHp",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def getRelevantInfo(tweet):\n",
    "    dictRelevantInfo ={}\n",
    "    data = json.loads(tweet)\n",
    "    hashtags = []\n",
    "    urlsList = []\n",
    "    text = ''\n",
    "    date = data['created_at'] ## ??? RT o no RT\n",
    "    try:\n",
    "        isRt=True\n",
    "        isRetweet=data[\"retweeted_status\"]\n",
    "        idTweet=isRetweet[\"id_str\"]\n",
    "        text = isRetweet['text']\n",
    "        username = isRetweet['user']['screen_name']\n",
    "        urls = isRetweet['entities']['urls']\n",
    "        rt_count = isRetweet['retweet_count']\n",
    "        likes = isRetweet['favorite_count']\n",
    "        \n",
    "        for h in isRetweet['entities']['hashtags']:\n",
    "            hashtags.append(h['text'])\n",
    "        for url in urls:\n",
    "            urlsList.append(url['url'])\n",
    "            \n",
    "    except:\n",
    "        isRt=False\n",
    "        idTweet=data[\"id_str\"]\n",
    "        text = data['text']\n",
    "        username = data['user']['screen_name']\n",
    "        urls = data['entities']['urls']\n",
    "        rt_count=data['retweet_count']\n",
    "        likes = data['favorite_count']\n",
    "        \n",
    "        for h in data['entities']['hashtags']:\n",
    "            hashtags.append(h['text'])\n",
    "            \n",
    "        for url in urls:\n",
    "            urlsList.append(url['url'])        \n",
    "            \n",
    "    \n",
    "    annonText=text.split()\n",
    "    for i in range(len(annonText)):\n",
    "        if annonText[i][0] == '@':\n",
    "            annonText[i] = str(hash(annonText[i]))\n",
    "    \n",
    "    dictRelevantInfo['tweetID'] = idTweet\n",
    "    dictRelevantInfo['text'] = \" \".join(annonText)\n",
    "    dictRelevantInfo['tokens'] = cleanTweet(text)\n",
    "    dictRelevantInfo['username'] = username\n",
    "    dictRelevantInfo['date'] = date\n",
    "    dictRelevantInfo['hashtags'] = hashtags\n",
    "    dictRelevantInfo['likes'] = likes\n",
    "    dictRelevantInfo['rt_count'] = rt_count\n",
    "    dictRelevantInfo['urlsList'] = urlsList\n",
    "    dictRelevantInfo['isRetweeted'] = isRt\n",
    "    return dictRelevantInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWx9_43aY9jC"
   },
   "source": [
    "#### cleanTweets dict & Drop Duplicates\n",
    "`Here we create a Dictionari (key::TweetID) of tweets. To do so, we iterate over the list of tweets from the dataset, preproces the tweet, and add it to the cleanTweets dictionary if it havent been added before (check for duplicates)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 49353,
     "status": "ok",
     "timestamp": 1607419308341,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -60
    },
    "id": "MqTtNufmSJHp"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-6a585643537c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcleanTweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcurrentTweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetRelevantInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtweetID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrentTweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweetID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Orignial tweet found, add to the dict or overwrite if retweet already exist.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-a67629723539>\u001b[0m in \u001b[0;36mgetRelevantInfo\u001b[0;34m(tweet)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdictRelevantInfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannonText\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mdictRelevantInfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleanTweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mdictRelevantInfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'username'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0musername\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mdictRelevantInfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-2d674c729b19>\u001b[0m in \u001b[0;36mcleanTweet\u001b[0;34m(tweetText)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcleanText\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleanText\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## Tokenize the text to get a list of terms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcleanText\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcleanText\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstops\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m##eliminate the stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcleanText\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemming\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcleanText\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m## perform stemming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleanText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcleanText\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'@'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-2d674c729b19>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcleanText\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleanText\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## Tokenize the text to get a list of terms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcleanText\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcleanText\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstops\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m##eliminate the stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcleanText\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemming\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcleanText\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m## perform stemming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleanText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcleanText\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'@'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36mstem\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step5a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step5b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36m_step4\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0mmeasure_gt_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_measure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         return self._apply_rule_list(\n\u001b[0m\u001b[1;32m    571\u001b[0m             \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             [\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36m_apply_rule_list\u001b[0;34m(self, word, rules)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mcondition\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mstem\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreplacement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(stem)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mtidying\u001b[0m \u001b[0mup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \"\"\"\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0mmeasure_gt_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_measure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         return self._apply_rule_list(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36m_measure\u001b[0;34m(self, stem)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m#      'architecture' becomes 'vcccvcvccvcv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consonant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m                 \u001b[0mcv_sequence\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36m_is_consonant\u001b[0;34m(self, word, i)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ma\u001b[0m \u001b[0mconsonant\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mvowel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \"\"\"\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvowels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"y\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cleanTweets = {}\n",
    "for t in tweets:\n",
    "    currentTweet = getRelevantInfo(t)\n",
    "    tweetID = currentTweet['tweetID']\n",
    "    # Orignial tweet found, add to the dict or overwrite if retweet already exist.\n",
    "    if currentTweet['isRetweeted'] == False:\n",
    "        cleanTweets[tweetID] = currentTweet\n",
    "    else:\n",
    "        if tweetID in cleanTweets:\n",
    "            continue\n",
    "        else:\n",
    "            cleanTweets[tweetID] = currentTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46864,
     "status": "ok",
     "timestamp": 1607419308342,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -60
    },
    "id": "ewXj_-iqSJHp",
    "outputId": "173126a0-7730-497b-af4d-1eb2874fc5d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of cleaned tweets:  38592\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of cleaned tweets: \",len(cleanTweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCscsGdCMRX6"
   },
   "source": [
    "## **Building the Search Engine**\n",
    "    Argument:    terms -- list of query terms\n",
    "                 docs -- list of documents, to rank, matching the query\n",
    "                 index -- inverted index data structure\n",
    "                 idf -- inverted document frequencies\n",
    "                 tf -- term frequencies\n",
    "                 titleIndex -- mapping between tweet id and tweet content\n",
    "    \n",
    "    Returns:     resultDocs -- list of tweetIDs in decreasing order and its score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biD9sERPENAQ"
   },
   "source": [
    "### Creating tf-idf dictionary\n",
    "\n",
    "Implement the inverted index and compute tf, df and idf\n",
    "\n",
    "\n",
    "\n",
    "    Argument:   cleanTweets -- collection of tweets\n",
    "                numTweets -- total number of tweets\n",
    "    \n",
    "    Returns:    index - the inverted index (implemented through a python dictionary) containing terms as keys and the corresponding \n",
    "                list of document these keys appears in (and the positions) as values.\n",
    "                tf - normalized term frequency for each term in each document\n",
    "                df - number of documents each term appear in\n",
    "                idf - inverse document frequency of each term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 738,
     "status": "ok",
     "timestamp": 1607420990284,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -60
    },
    "id": "FMjGU0ZbTAQj"
   },
   "outputs": [],
   "source": [
    "def create_index_tfidf(cleanTweets, numTweets):\n",
    "    # lines -> cleanTweets\n",
    "    # numDocs -> numOfTweets\n",
    "        \n",
    "    index=defaultdict(list)\n",
    "    tf=defaultdict(list)        #term frequencies of terms in tweets\n",
    "    df=defaultdict(int)         #document frequencies of terms in the corpus\n",
    "    titleIndex=defaultdict(str)\n",
    "    idf=defaultdict(float)\n",
    "\n",
    "    tweetIndex=defaultdict(float)\n",
    "    \n",
    "    for line in cleanTweets.values(): # Remember, cleanTweets contain all tweets, each line is a tweet\n",
    "        tweetID = line['tweetID']        \n",
    "        terms = line['tokens']\n",
    "        tweetIndex[tweetID]=line['text'] \n",
    "\n",
    "        termdictPage={}\n",
    "        for position, term in enumerate(terms): ## terms contains all the tokens of the actual tweet\n",
    "            try:\n",
    "                # if the term is already in the dict append the position to the corrisponding list\n",
    "                termdictPage[term][tweetID].append(position) \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                termdictPage[term]=[tweetID, array('I',[position])] #'I' indicates unsigned int (int in python)\n",
    "        \n",
    "        # normalize term frequencies\n",
    "        # Compute the denominator to normalize term frequencies\n",
    "        # norm is the same for all terms of a document.\n",
    "        norm=0\n",
    "        for term, posting in termdictPage.items(): \n",
    "            # posting is a list containing tweetID and the list of positions for current term in current tweet: \n",
    "            # posting ==> [tweetID, [list of positions]] \n",
    "            norm+=len(posting[1])**2\n",
    "        norm=math.sqrt(norm)\n",
    "\n",
    "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in termdictPage.items():     \n",
    "            # append the tf for current term (tf = term frequency in current tweet/norm)\n",
    "            tf[term].append(np.round(len(posting[1])/norm,4))\n",
    "            #increment the tweet frequency of current term (number of tweets containing the current term)\n",
    "            df[term]= len(termdictPage[term])  # increment df for current term\n",
    "        \n",
    "        #merge the current page index with the main index\n",
    "        for termpage, postingpage in termdictPage.items():\n",
    "            index[termpage].append(postingpage)\n",
    "            \n",
    "    # Compute idf\n",
    "    for term in df:\n",
    "        idf[term] = np.round(np.log(float(numTweets/df[term])),4)\n",
    "            \n",
    "    return index, tf, df, idf, tweetIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7158,
     "status": "ok",
     "timestamp": 1607420996710,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -60
    },
    "id": "Mvl-m4H-TAQl",
    "outputId": "fe17d76f-4c58-4050-afa8-bdb21c6f3964"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 5.95 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "numTweets = len(cleanTweets)\n",
    "index, tf, df, idf, tweetIndex = create_index_tfidf(cleanTweets, numTweets)\n",
    "print(\"Total time to create the index: {} seconds\" .format(np.round(time.time() - start_time,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P__OBmrNEaui"
   },
   "source": [
    "### Ranking tweets based on TF-IDFs + Cosine Similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 7154,
     "status": "ok",
     "timestamp": 1607420996710,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -60
    },
    "id": "2pir0XHJTAQp"
   },
   "outputs": [],
   "source": [
    "def rankDocuments_TFIDF(terms, docs, index, idf, tf, titleIndex):\n",
    "            \n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    # The remaing elements would became 0 when multiplied to the queryVector\n",
    "    docVectors=defaultdict(lambda: [0]*len(terms)) # I call docVectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary \n",
    "    queryVector=[0]*len(terms)    \n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms) # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
    "    \n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "    \n",
    "    for termIndex, term in enumerate(terms): #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "                    \n",
    "        ## Compute tf*idf(normalize tf as done with documents)\n",
    "        queryVector[termIndex]=query_terms_count[term]/query_norm * idf[term]\n",
    "\n",
    "        # Generate docVectors for matching docs\n",
    "        for docIndex, (doc, postings) in enumerate(index[term]):\n",
    "            if doc in docs:\n",
    "                docVectors[doc][termIndex]=tf[term][docIndex] * idf[term]\n",
    "\n",
    "    # calculate the score of each tweet\n",
    "    # compute the cosine similarity between queyVector and each docVector:    \n",
    "    docScores=[ [np.dot(curDocVec, queryVector)/(np.linalg.norm(curDocVec)*np.linalg.norm(queryVector)), doc] for doc, curDocVec in docVectors.items() ]\n",
    "    docScores.sort(reverse=True)\n",
    "    resultDocs=[(x[0], x[1]) for x in docScores]\n",
    "    if len(resultDocs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)  \n",
    "    return resultDocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23_nPV1RLBMM"
   },
   "source": [
    "### Ranking tweets based on TF-IDFs + Likes + Retweets + Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 781,
     "status": "ok",
     "timestamp": 1607421044106,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -60
    },
    "id": "NqsBn4ykLDX3"
   },
   "outputs": [],
   "source": [
    "def createLikeRTIndex(cleanTweets):\n",
    "    for tweet in cleanTweets:\n",
    "        cleanTweets[tweet]['likes_score'] = (-np.exp(-(cleanTweets[tweet]['likes']/50000))+1)\n",
    "        cleanTweets[tweet]['rt_score'] = (-np.exp(-(cleanTweets[tweet]['rt_count']/25000))+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 1065,
     "status": "ok",
     "timestamp": 1607421046363,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -60
    },
    "id": "h_EfX8hJLLnz"
   },
   "outputs": [],
   "source": [
    "def rankDocuments_Likes_Retweets(terms, docs, index, idf, tf, titleIndex):\n",
    "            \n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    # The remaing elements would became 0 when multiplied to the queryVector\n",
    "    docVectors=defaultdict(lambda: [0]*len(terms)) # I call docVectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary \n",
    "    queryVector=[0]*len(terms)    \n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms) # get the frequency of each term in the query. \n",
    "    \n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "    \n",
    "    for termIndex, term in enumerate(terms): #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "                    \n",
    "        ## Compute tf*idf(normalize tf as done with documents)\n",
    "        queryVector[termIndex]=query_terms_count[term]/query_norm * idf[term]\n",
    "\n",
    "        # Generate docVectors for matching docs\n",
    "        for docIndex, (doc, postings) in enumerate(index[term]):\n",
    "            \n",
    "            if doc in docs:\n",
    "                docVectors[doc][termIndex]=tf[term][docIndex] * idf[term]\n",
    "\n",
    "    # calculate the score of each tweet\n",
    "    # compute the cosine similarity between queyVector and each docVector:\n",
    "       \n",
    "    docScores=[ [(np.dot(curDocVec, queryVector)/(np.linalg.norm(curDocVec)*np.linalg.norm(queryVector)))*0.6+cleanTweets[doc]['likes_score']*0.2+cleanTweets[doc]['rt_score']*0.2, doc] for doc, curDocVec in docVectors.items() ]\n",
    "    docScores.sort(reverse=True)\n",
    "    resultDocs=[(x[0], x[1]) for x in docScores]\n",
    "    #print document titles instead if document id's\n",
    "    #resultDocs=[ titleIndex[x] for x in resultDocs ]\n",
    "    if len(resultDocs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)    \n",
    "    #print ('\\n'.join(resultDocs), '\\n')\n",
    "    return resultDocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrUqrRoIIT1a"
   },
   "source": [
    "## **Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 899,
     "status": "ok",
     "timestamp": 1607421049175,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -60
    },
    "id": "CLdEEhT1TAQr"
   },
   "outputs": [],
   "source": [
    "def search_tf_idf(query, index, ranking_type = '0'):\n",
    "    '''\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    '''\n",
    "    query=cleanTweet(query)\n",
    "    docs=set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in termDocs the ids of the docs that contain \"term\"                        \n",
    "            termDocs=[posting[0] for posting in index[term]]\n",
    "            \n",
    "            # docs = docs Union termDocs\n",
    "            docs = docs.union(termDocs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs=list(docs)\n",
    "    if ranking_type == '0': # TF-IDF\n",
    "        ranked_docs = rankDocuments_TFIDF(query, docs, index, idf, tf, tweetIndex)   \n",
    "    elif ranking_type == '1': # TF-IDF + Likes + Retweets\n",
    "        ranked_docs = rankDocuments_Likes_Retweets(query, docs, index, idf, tf, tweetIndex)\n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EpTgX-pTTAQu"
   },
   "outputs": [],
   "source": [
    "print(\"Insert your query:\\n\")\n",
    "query = input()\n",
    "ranked_docs = search_tf_idf(query, index, ranking_type = '0')    \n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the seached query:\\n\".format(top, len(ranked_docs)))\n",
    "for d_score, d_id in ranked_docs[:top] :\n",
    "    print(\"Tweet ID= {}\\nTweet: {}\\nScore: {}\\n\".format(d_id, tweetIndex[d_id], round(d_score, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ouLxlFgf9PL1"
   },
   "source": [
    "## Store TOP-20 tweets from the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 12694,
     "status": "ok",
     "timestamp": 1607425201228,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -60
    },
    "id": "Rdeq0u_47WJO"
   },
   "outputs": [],
   "source": [
    "list_of_queries = ['GOP wins elections?',\n",
    "                   'The symbol of the GOP is an elephant.',\n",
    "                   'Trump says elections have been corrupted',\n",
    "                   'Biden takes the action.',\n",
    "                   'fraud republican election',\n",
    "                   'Kamala Harris will be the first woman Vice President of the US.',\n",
    "                   'Biden will fight climate change',\n",
    "                   'Can I see elections on netflix?',\n",
    "                   'thanks realdonaldtrump',\n",
    "                   \"Trump's attempt to steal the election unravels as coronavirus cases surge\"]\n",
    "top = 20\n",
    "for i in range(10):\n",
    "    query = list_of_queries[i]\n",
    "    ranked_docs = search_tf_idf(query, index, ranking_type = '0')    \n",
    "\n",
    "    with open(data_path+'output/Query_'+str(i)+'.tsv', 'wt') as out_file:\n",
    "        tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "        tsv_writer.writerow(['Ranking:', 'Tweet:', 'Score:'])\n",
    "        for i, (d_score, d_id) in enumerate(ranked_docs[:top]):\n",
    "            tsv_writer.writerow([i, tweetIndex[d_id], round(d_score, 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sum_of_squared_distances = []\n",
    "K = range(10,30)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(fin)\n",
    "    Sum_of_squareddistances.append(km.inertia)\n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6PDgndzJeurq",
    "MlNg4TFXTAQI",
    "_A3K2wbqUJ9B",
    "ryhRZ8N-VTHI",
    "zCscsGdCMRX6",
    "biD9sERPENAQ",
    "P__OBmrNEaui"
   ],
   "name": "Project_IRWA_Part_2_Build_Search_Engine.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
