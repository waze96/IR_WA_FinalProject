{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHK-yP0fTAP9"
   },
   "source": [
    "# Information Retrieval and Web Analytics: Indexing + Modeling (TF-IDF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20161,
     "status": "ok",
     "timestamp": 1607419222673,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -60
    },
    "id": "8F9PVJlbTAQF",
    "outputId": "e87718b0-cfea-4cbc-dd42-502c551d4b39"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/waze/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/waze/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import nltk\n",
    "nltk.download('stopwords');\n",
    "nltk.download('punkt') # used in sent_tokenize\n",
    "\n",
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.manifold import TSNE\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib.pyplot import cm\n",
    "\n",
    "from gensim.summarization.bm25 import BM25\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "\n",
    "data_path = '../one100K_v2.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PDgndzJeurq"
   },
   "source": [
    "## **Preprocessing Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlNg4TFXTAQI"
   },
   "source": [
    "#### Load data into memory\n",
    "\n",
    "The dataset ```dataset.jason``` contains a list of N tweets and its information. This dataset has ben made scrapping for tweets that contain any of the words \"Trump\", \"#Trump\", \"Biden\", \"#Biden\", \"#UsElections2020\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 8791,
     "status": "ok",
     "timestamp": 1607419247050,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -60
    },
    "id": "X4P3KzRMTAQJ"
   },
   "outputs": [],
   "source": [
    "docs_path = data_path\n",
    "with open(docs_path) as fp:\n",
    "    lines = fp.readlines()\n",
    "tweets = [l.strip().replace(' +', ' ') for l in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_A3K2wbqUJ9B"
   },
   "source": [
    "#### Clean Tweets\n",
    "Preprocess the text of a concrete tweet removing non alphabetic characters, stop words, stemming, transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:  tweetText -- string (text) to be preprocessed    \n",
    "    Returns:   cleanText - a list of tokens corresponding to the tweetText after the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1089,
     "status": "ok",
     "timestamp": 1607419254114,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -60
    },
    "id": "6Bw4AjprSJHp"
   },
   "outputs": [],
   "source": [
    "def cleanTweet(tweetText):      \n",
    "    stemming = PorterStemmer()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "    cleanText = tweetText.lower() ## Transform in lowercase\n",
    "    cleanText = re.sub('[:\\[\\]&%$\\\"\\'!./,;:?=¿^\\-_*+)<>(¡@]','',cleanText)\n",
    "    cleanText = cleanText.split() ## Tokenize the text to get a list of terms\n",
    "    cleanText = [word for word in cleanText if word not in stops]  ##eliminate the stopwords\n",
    "    cleanText = [stemming.stem(word) for word in cleanText] ## perform stemming\n",
    "    return cleanText\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryhRZ8N-VTHI"
   },
   "source": [
    "#### Crate a dictionary for each tweet\n",
    "For each tweet, creates a dictionary containing the most relevant information of it (Username, OriginalText, Clean Tokens, number of Likes, number of retweets, list of URLs...)\n",
    "    \n",
    "    Argument:  tweet -- a JSON tweet content    \n",
    "    Returns:   dictRelevantInfo -- a dictionary with the processed tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1007,
     "status": "ok",
     "timestamp": 1607419256787,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -60
    },
    "id": "_dUAcWTaSJHp",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def getRelevantInfo(tweet):\n",
    "    dictRelevantInfo ={}\n",
    "    data = json.loads(tweet)\n",
    "    hashtags = []\n",
    "    urlsList = []\n",
    "    text = ''\n",
    "    date = data['created_at'] ## ??? RT o no RT\n",
    "    try:\n",
    "        isRt=True\n",
    "        isRetweet=data[\"retweeted_status\"]\n",
    "        idTweet=isRetweet[\"id_str\"]\n",
    "        text = isRetweet['text']\n",
    "        username = isRetweet['user']['screen_name']\n",
    "        urls = isRetweet['entities']['urls']\n",
    "        rt_count = isRetweet['retweet_count']\n",
    "        likes = isRetweet['favorite_count']\n",
    "        \n",
    "        for h in isRetweet['entities']['hashtags']:\n",
    "            hashtags.append(h['text'])\n",
    "        for url in urls:\n",
    "            urlsList.append(url['url'])\n",
    "            \n",
    "    except:\n",
    "        isRt=False\n",
    "        idTweet=data[\"id_str\"]\n",
    "        text = data['text']\n",
    "        username = data['user']['screen_name']\n",
    "        urls = data['entities']['urls']\n",
    "        rt_count=data['retweet_count']\n",
    "        likes = data['favorite_count']\n",
    "        \n",
    "        for h in data['entities']['hashtags']:\n",
    "            hashtags.append(h['text'])\n",
    "            \n",
    "        for url in urls:\n",
    "            urlsList.append(url['url'])        \n",
    "            \n",
    "    dictRelevantInfo['tweetID'] = idTweet\n",
    "    dictRelevantInfo['text'] = text\n",
    "    dictRelevantInfo['tokens'] = cleanTweet(text)\n",
    "    dictRelevantInfo['username'] = username\n",
    "    dictRelevantInfo['date'] = date\n",
    "    dictRelevantInfo['hashtags'] = hashtags\n",
    "    dictRelevantInfo['likes'] = likes\n",
    "    dictRelevantInfo['rt_count'] = rt_count\n",
    "    dictRelevantInfo['urlsList'] = urlsList\n",
    "    dictRelevantInfo['isRetweeted'] = isRt\n",
    "    return dictRelevantInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWx9_43aY9jC"
   },
   "source": [
    "#### cleanTweets dict & Drop Duplicates\n",
    "`Here we create a Dictionari (key::TweetID) of tweets. To do so, we iterate over the list of tweets from the dataset, preproces the tweet, and add it to the cleanTweets dictionary if it havent been added before (check for duplicates)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 49353,
     "status": "ok",
     "timestamp": 1607419308341,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -60
    },
    "id": "MqTtNufmSJHp"
   },
   "outputs": [],
   "source": [
    "cleanTweets = {}\n",
    "for t in tweets:\n",
    "    currentTweet = getRelevantInfo(t)\n",
    "    tweetID = currentTweet['tweetID']\n",
    "    # Orignial tweet found, add to the dict or overwrite if retweet already exist.\n",
    "    if currentTweet['isRetweeted'] == False:\n",
    "        cleanTweets[tweetID] = currentTweet\n",
    "    else:\n",
    "        if tweetID in cleanTweets:\n",
    "            continue\n",
    "        else:\n",
    "            cleanTweets[tweetID] = currentTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46864,
     "status": "ok",
     "timestamp": 1607419308342,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -60
    },
    "id": "ewXj_-iqSJHp",
    "outputId": "173126a0-7730-497b-af4d-1eb2874fc5d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of cleaned tweets:  38592\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of cleaned tweets: \",len(cleanTweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCscsGdCMRX6"
   },
   "source": [
    "## **Building the Search Engine**\n",
    "    Argument:    terms -- list of query terms\n",
    "                 docs -- list of documents, to rank, matching the query\n",
    "                 index -- inverted index data structure\n",
    "                 idf -- inverted document frequencies\n",
    "                 tf -- term frequencies\n",
    "                 titleIndex -- mapping between tweet id and tweet content\n",
    "    \n",
    "    Returns:     resultDocs -- list of tweetIDs in decreasing order and its score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biD9sERPENAQ"
   },
   "source": [
    "### Creating tf-idf dictionary\n",
    "\n",
    "Implement the inverted index and compute tf, df and idf\n",
    "\n",
    "\n",
    "\n",
    "    Argument:   cleanTweets -- collection of tweets\n",
    "                numTweets -- total number of tweets\n",
    "    \n",
    "    Returns:    index - the inverted index (implemented through a python dictionary) containing terms as keys and the corresponding \n",
    "                list of document these keys appears in (and the positions) as values.\n",
    "                tf - normalized term frequency for each term in each document\n",
    "                df - number of documents each term appear in\n",
    "                idf - inverse document frequency of each term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 738,
     "status": "ok",
     "timestamp": 1607420990284,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -60
    },
    "id": "FMjGU0ZbTAQj"
   },
   "outputs": [],
   "source": [
    "def create_index_tfidf(cleanTweets, numTweets):\n",
    "    # lines -> cleanTweets\n",
    "    # numDocs -> numOfTweets\n",
    "        \n",
    "    index=defaultdict(list)\n",
    "    tf=defaultdict(list)        #term frequencies of terms in tweets\n",
    "    df=defaultdict(int)         #document frequencies of terms in the corpus\n",
    "    titleIndex=defaultdict(str)\n",
    "    idf=defaultdict(float)\n",
    "\n",
    "    tweetIndex=defaultdict(float)\n",
    "    \n",
    "    for line in cleanTweets.values(): # Remember, cleanTweets contain all tweets, each line is a tweet\n",
    "        tweetID = line['tweetID']        \n",
    "        terms = line['tokens']\n",
    "        tweetIndex[tweetID]=line['text'] \n",
    "\n",
    "        termdictPage={}\n",
    "        for position, term in enumerate(terms): ## terms contains all the tokens of the actual tweet\n",
    "            try:\n",
    "                # if the term is already in the dict append the position to the corrisponding list\n",
    "                termdictPage[term][tweetID].append(position) \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                termdictPage[term]=[tweetID, array('I',[position])] #'I' indicates unsigned int (int in python)\n",
    "        \n",
    "        # normalize term frequencies\n",
    "        # Compute the denominator to normalize term frequencies\n",
    "        # norm is the same for all terms of a document.\n",
    "        norm=0\n",
    "        for term, posting in termdictPage.items(): \n",
    "            # posting is a list containing tweetID and the list of positions for current term in current tweet: \n",
    "            # posting ==> [tweetID, [list of positions]] \n",
    "            norm+=len(posting[1])**2\n",
    "        norm=math.sqrt(norm)\n",
    "\n",
    "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in termdictPage.items():     \n",
    "            # append the tf for current term (tf = term frequency in current tweet/norm)\n",
    "            tf[term].append(np.round(len(posting[1])/norm,4))\n",
    "            #increment the tweet frequency of current term (number of tweets containing the current term)\n",
    "            df[term]= len(termdictPage[term])  # increment df for current term\n",
    "        \n",
    "        #merge the current page index with the main index\n",
    "        for termpage, postingpage in termdictPage.items():\n",
    "            index[termpage].append(postingpage)\n",
    "            \n",
    "    # Compute idf\n",
    "    for term in df:\n",
    "        idf[term] = np.round(np.log(float(numTweets/df[term])),4)\n",
    "            \n",
    "    return index, tf, df, idf, tweetIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7158,
     "status": "ok",
     "timestamp": 1607420996710,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -60
    },
    "id": "Mvl-m4H-TAQl",
    "outputId": "fe17d76f-4c58-4050-afa8-bdb21c6f3964"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 5.01 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "numTweets = len(cleanTweets)\n",
    "index, tf, df, idf, tweetIndex = create_index_tfidf(cleanTweets, numTweets)\n",
    "print(\"Total time to create the index: {} seconds\" .format(np.round(time.time() - start_time,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P__OBmrNEaui"
   },
   "source": [
    "### Ranking tweets based on TF-IDFs + Cosine Similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 7154,
     "status": "ok",
     "timestamp": 1607420996710,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -60
    },
    "id": "2pir0XHJTAQp"
   },
   "outputs": [],
   "source": [
    "def rankDocuments_TFIDF(terms, docs, index, idf, tf, titleIndex):\n",
    "            \n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    # The remaing elements would became 0 when multiplied to the queryVector\n",
    "    docVectors=defaultdict(lambda: [0]*len(terms)) # I call docVectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary \n",
    "    queryVector=[0]*len(terms)    \n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms) # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
    "    \n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "    \n",
    "    for termIndex, term in enumerate(terms): #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "                    \n",
    "        ## Compute tf*idf(normalize tf as done with documents)\n",
    "        queryVector[termIndex]=query_terms_count[term]/query_norm * idf[term]\n",
    "\n",
    "        # Generate docVectors for matching docs\n",
    "        for docIndex, (doc, postings) in enumerate(index[term]):\n",
    "            if doc in docs:\n",
    "                docVectors[doc][termIndex]=tf[term][docIndex] * idf[term]\n",
    "\n",
    "    # calculate the score of each tweet\n",
    "    # compute the cosine similarity between queyVector and each docVector:    \n",
    "    docScores=[ [np.dot(curDocVec, queryVector)/(np.linalg.norm(curDocVec)*np.linalg.norm(queryVector)), doc] for doc, curDocVec in docVectors.items() ]\n",
    "    docScores.sort(reverse=True)\n",
    "    resultDocs=[(x[0], x[1]) for x in docScores]\n",
    "    if len(resultDocs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)  \n",
    "    return resultDocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23_nPV1RLBMM"
   },
   "source": [
    "### Ranking tweets based on TF-IDFs + Likes + Retweets + Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 781,
     "status": "ok",
     "timestamp": 1607421044106,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -60
    },
    "id": "NqsBn4ykLDX3"
   },
   "outputs": [],
   "source": [
    "def createLikeRTIndex(cleanTweets):\n",
    "    # maxLikes = 0\n",
    "    # maxRT = 1\n",
    "    # for tweet in lines:\n",
    "    #     if lines[tweet]['likes'] > maxLikes:\n",
    "    #         maxLikes = lines[tweet]['likes']\n",
    "    #     if lines[tweet]['rt_count'] > maxRT:\n",
    "    #         maxRT = lines[tweet]['rt_count']\n",
    "    for tweet in cleanTweets:\n",
    "        cleanTweets[tweet]['likes_score'] = (-np.exp(-(cleanTweets[tweet]['likes']/50000))+1)\n",
    "        cleanTweets[tweet]['rt_score'] = (-np.exp(-(cleanTweets[tweet]['rt_count']/25000))+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 1065,
     "status": "ok",
     "timestamp": 1607421046363,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -60
    },
    "id": "h_EfX8hJLLnz"
   },
   "outputs": [],
   "source": [
    "def rankDocuments_Likes_Retweets(terms, docs, index, idf, tf, titleIndex):\n",
    "            \n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    # The remaing elements would became 0 when multiplied to the queryVector\n",
    "    docVectors=defaultdict(lambda: [0]*len(terms)) # I call docVectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary \n",
    "    queryVector=[0]*len(terms)    \n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms) # get the frequency of each term in the query. \n",
    "    \n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "    \n",
    "    for termIndex, term in enumerate(terms): #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "                    \n",
    "        ## Compute tf*idf(normalize tf as done with documents)\n",
    "        queryVector[termIndex]=query_terms_count[term]/query_norm * idf[term]\n",
    "\n",
    "        # Generate docVectors for matching docs\n",
    "        for docIndex, (doc, postings) in enumerate(index[term]):\n",
    "            \n",
    "            if doc in docs:\n",
    "                docVectors[doc][termIndex]=tf[term][docIndex] * idf[term]\n",
    "\n",
    "    # calculate the score of each tweet\n",
    "    # compute the cosine similarity between queyVector and each docVector:\n",
    "       \n",
    "    docScores=[ [(np.dot(curDocVec, queryVector)/(np.linalg.norm(curDocVec)*np.linalg.norm(queryVector)))*0.6+cleanTweets[doc]['likes_score']*0.2+cleanTweets[doc]['rt_score']*0.2, doc] for doc, curDocVec in docVectors.items() ]\n",
    "    docScores.sort(reverse=True)\n",
    "    resultDocs=[(x[0], x[1]) for x in docScores]\n",
    "    #print document titles instead if document id's\n",
    "    #resultDocs=[ titleIndex[x] for x in resultDocs ]\n",
    "    if len(resultDocs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)    \n",
    "    #print ('\\n'.join(resultDocs), '\\n')\n",
    "    return resultDocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrUqrRoIIT1a"
   },
   "source": [
    "## **Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 899,
     "status": "ok",
     "timestamp": 1607421049175,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -60
    },
    "id": "CLdEEhT1TAQr"
   },
   "outputs": [],
   "source": [
    "def search_tf_idf(query, index, ranking_type = '0'):\n",
    "    '''\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    '''\n",
    "    query=cleanTweet(query)\n",
    "    docs=set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in termDocs the ids of the docs that contain \"term\"                        \n",
    "            termDocs=[posting[0] for posting in index[term]]\n",
    "            \n",
    "            # docs = docs Union termDocs\n",
    "            docs = docs.union(termDocs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs=list(docs)\n",
    "    if ranking_type == '0': # TF-IDF\n",
    "        ranked_docs = rankDocuments_TFIDF(query, docs, index, idf, tf, tweetIndex)   \n",
    "    elif ranking_type == '1': # TF-IDF + Likes + Retweets\n",
    "        ranked_docs = rankDocuments_Likes_Retweets(query, docs, index, idf, tf, tweetIndex)\n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(docs, preprocess=preprocess_string, verbose=10000):\n",
    "    \n",
    "    for i, doc in enumerate(docs):\n",
    "        yield preprocess(doc) # preprocess\n",
    "        \n",
    "        # print progress if needed\n",
    "        if verbose > 0 and (i + 1) % verbose == 0:\n",
    "            print(f\"Progress: {i + 1}\")\n",
    "            \n",
    "def get_sentences(docs, verbose=10000):\n",
    "    #loop over all docs (tweets in our case)\n",
    "    for i, doc in enumerate(docs):\n",
    "        \n",
    "        # use nltk.sent_tokenize to split paragraphs into sentences\n",
    "        for sentence in nltk.sent_tokenize(doc):\n",
    "            # preprocess each sentence using gensim (return string not list)\n",
    "            yield \" \".join(preprocess_string(sentence))\n",
    "            \n",
    "        # print progress if needed\n",
    "        if verbose > 0 and (i + 1) % verbose == 0:\n",
    "            print(f\"Progress: {i + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 10000\n",
      "Progress: 20000\n",
      "Progress: 30000\n",
      "Progress: 40000\n",
      "Progress: 50000\n",
      "Progress: 60000\n",
      "Progress: 70000\n",
      "Progress: 80000\n",
      "Progress: 90000\n",
      "Progress: 100000\n"
     ]
    }
   ],
   "source": [
    "sentences = list(get_sentences(tweets))\n",
    "#split each sentence into a list od words\n",
    "words = [s.split() for s in sentences]\n",
    "#create a word2Vec model  \n",
    "MODEL_DIMENSION = 70\n",
    "w2v_model = Word2Vec(sentences = words, size=MODEL_DIMENSION, window=10, min_count=1, negative=15, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query(query, wv, topn=10):\n",
    "    \n",
    "    query = preprocess_string(query)\n",
    "    expanded_query = [t for t in query] # initialize with original query. Note, it is a list\n",
    "    \n",
    "    # extend each single term of the original query and append to expanded query\n",
    "    for t in query:\n",
    "        expanded_query.extend(s for s, f in wv.most_similar(t, topn=topn))\n",
    "        \n",
    "    return expanded_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(bm25, query, n=100):\n",
    "    #apply preprocessing to the query using get_tokens and tranform it from string to list of terms\n",
    "    query = query.split() # cast query from string to list\n",
    "    query = list(get_tokens(query)) # apply preprocessing\n",
    "    \n",
    "    query = [item for sublist in query for item in sublist] # transform list of list to list\n",
    "    \n",
    "    # score docs using a specific function of bm25        \n",
    "    scores = np.array(bm25.get_scores(query))\n",
    "        \n",
    "    # get indices of top N scores\n",
    "    idx = np.argpartition(scores, -n)[-n:] # INDEX DELS TOP 20\n",
    "    \n",
    "    # sort top N scores and return their indices\n",
    "    # if all the scores are 0 return empty list\n",
    "    if np.sum(scores[idx]) == 0: \n",
    "        return[] \n",
    "    return idx[np.argsort(-scores[idx])], scores[idx[np.argsort(-scores[idx])]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play with some queries - insert also query with typos\n",
    "bm25 = BM25(tokens) # constructing a paragraph search index\n",
    "expanded_query = ''\n",
    "query = input()\n",
    "try:\n",
    "    expanded_query = ' '.join(expand_query(query, w2v_model))\n",
    "except Exception as e:\n",
    "    print(\"asdasd\")\n",
    "    print(e)\n",
    "\n",
    "top_idx = None\n",
    "print('Original query: {}'.format(query))\n",
    "print('Expanded query: {}'.format(expanded_query))\n",
    "print('---')\n",
    "try:\n",
    "    indexTmp, score = get_top_n(bm25, expanded_query, n=100)\n",
    "    data = {'id':  [], 'text': [], 'scoreW2V': [], 'score2': []}\n",
    "    df = pd.DataFrame (data, columns = ['id','text', 'scoreW2V', 'score2', 'totalScore'])\n",
    "    print(\"estamoss1\")\n",
    "    for i in zip(indexTmp,score):\n",
    "        \n",
    "        idx=i[0]\n",
    "        score=i[1]\n",
    "        df = df.append({'id': tweetsID[idx], 'text': tweets[idx], 'scoreW2V': score}, ignore_index=True)\n",
    "    print(\"estamoss2\")    \n",
    "    ranked_docs = search_tf_idf(query, index) \n",
    "    print(\"estamoss3\")\n",
    "    for score, idx in ranked_docs:\n",
    "        df['score2'][df['id'] == idx] = score  \n",
    "    print(\"estamoss4\")    \n",
    "    df = df.fillna(0)\n",
    "    df['totalScore'] = df['scoreW2V'] + df['score2']\n",
    "    df = df.sort_values(by=['totalScore'], ascending=False)\n",
    "    dfList=df.values.tolist()\n",
    "    for x in dfList[:20]:\n",
    "        print('\\nTweet ID: {}'.format(x[0]))\n",
    "        print('\\nTweet: {}'.format(x[1]))\n",
    "        print('\\nScore: {}\\n'.format(x[4]))\n",
    "        print('*****************************')\n",
    "except:\n",
    "    print(\"No matching documents found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_vector(tweet, model=w2v_model):\n",
    "    query_vec=[]\n",
    "    i=0\n",
    "    ## AQUI TENIEN EL TWEET EN TOKENS\n",
    "    for word in tweet:\n",
    "        if word in w2v_model.wv.vocab:\n",
    "            query_vec.append(w2v_model.wv.word_vec(word))\n",
    "            i+=1\n",
    "\n",
    "    if i!=0:\n",
    "        vec=np.average(np.array(query_vec), axis=0)\n",
    "        return vec/np.linalg.norm(vec)\n",
    "\n",
    "    #print(tweet)\n",
    "    return np.zeros((MODEL_DIMENSION,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_tweets=[]\n",
    "[embedded_tweets.append((emb_vector(cleanTweets[tweetID]['tokens']),tweetID)) for tweetID in cleanTweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin=[vec[0] for vec in embedded_tweets]\n",
    "low_dimension_embedded = TSNE(n_components=2, perplexity=2, random_state=33).fit_transform(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLUSTERS=3\n",
    "\n",
    "colors_list=cm.rainbow(np.linspace(0,1,NUM_CLUSTERS))\n",
    "\n",
    "kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=0).fit(fin)\n",
    "print(kmeans.labels_)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('T-SNE Representation of Word2Vec')\n",
    "\n",
    "for i in range(0,NUM_CLUSTERS):\n",
    "    \n",
    "    sns.scatterplot(x=low_dimension_embedded[kmeans.labels_==i,0],y=low_dimension_embedded[kmeans.labels_==i,1],color=colors_list[i][0:3],label=\"cluster \"+str(i))\n",
    "\n",
    "\n",
    "#sns.scatterplot(x=low_dimension_embedded[:,0], y=low_dimension_embedded[:,1], legend='full',  c=kmeans.labels_)\n",
    "\n",
    "#sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y, legend='full', palette=palette)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "np.save('tsne_output_acabat.npy', low_dimension_embedded)\n",
    "#low_dim_embedded=np.load('tsne_output_acabat.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,tweetID in enumerate(cleanTweets):\n",
    "    cleanTweets[tweetID][\"cluster\"]=kmeans.labels_[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = kmeans.cluster_centers_\n",
    "print(centroids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14829,
     "status": "ok",
     "timestamp": 1607421065374,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -60
    },
    "id": "EpTgX-pTTAQu",
    "outputId": "3b5c2d92-13d8-4aad-8fda-f38e3e6e04c5"
   },
   "outputs": [],
   "source": [
    "### QUERY AMB EL NOSTRE == 1!!\n",
    "print(\"Insert your query:\\n\")\n",
    "query = input()\n",
    "ranked_docs = search_tf_idf(query, index, ranking_type = '1')    \n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the seached query:\\n\".format(top, len(ranked_docs)))\n",
    "for d_score, d_id in ranked_docs[:top] :\n",
    "    print(\"Tweet ID= {}\\nTweet: {}\\nScore: {}\\n\".format(d_id, tweetIndex[d_id], round(d_score, 4)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "biD9sERPENAQ",
    "P__OBmrNEaui"
   ],
   "name": "Project_IRWA_Part_2_Build_Search_Engine.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
