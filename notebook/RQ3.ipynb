{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import json\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "data_path = '../data/one100K_v2.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simply we load the data from our scrapped tweets file: one100K_v2.json to a list tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD OUR DATA FROM THE SCRAPPED TWEETS\n",
    "docs_path = data_path\n",
    "with open(docs_path) as fp:\n",
    "    lines = fp.readlines()\n",
    "tweets = [l.strip().replace(' +', ' ') for l in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total numer of tweets in the corpus: {}\" .format(len(tweets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dictionary for each tweet\n",
    "For each tweet, creates a dictionary containing the most relevant information of it (Username, OriginalText, Clean Tokens, number of Likes, number of retweets, list of URLs...)\n",
    "    \n",
    "    Argument:  tweet -- a JSON tweet content    \n",
    "    Returns:   dictRelevantInfo -- a dictionary with the processed tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelevantInfo(tweet):\n",
    "    dictRelevantInfo ={}\n",
    "    data = json.loads(tweet)\n",
    "    hashtags = []\n",
    "    urlsList = []\n",
    "    text = ''\n",
    "    date = data['created_at'] \n",
    "    \n",
    "    ## TRY TO OBTAIN INFORMATION ABOUT THE RETWEETED TWEET IF 'tweet' IS A RT.\n",
    "    try:\n",
    "        isRt=True\n",
    "        isRetweet=data[\"retweeted_status\"]\n",
    "        idTweet=data[\"id_str\"]\n",
    "        text = isRetweet['text']\n",
    "        usernamert=isRetweet['user']['screen_name']\n",
    "        username = data['user']['screen_name']\n",
    "        urls = isRetweet['entities']['urls']\n",
    "        rt_count = isRetweet['retweet_count']\n",
    "        likes = isRetweet['favorite_count']\n",
    "        id_retweet=isRetweet[\"id_str\"]\n",
    "        for h in isRetweet['entities']['hashtags']:\n",
    "            hashtags.append(h['text'])\n",
    "        for url in urls:\n",
    "            urlsList.append(url['url'])\n",
    "            \n",
    "    ## IF THE CURRENT TWEET IS NOT A RT WE OBTAIN INFORMATION ABOUT THE ORIGINAL.\n",
    "    except:\n",
    "        isRt=False\n",
    "        idTweet=data[\"id_str\"]\n",
    "        text = data['text']\n",
    "        username = data['user']['screen_name']\n",
    "        urls = data['entities']['urls']\n",
    "        rt_count=data['retweet_count']\n",
    "        likes = data['favorite_count']\n",
    "        id_retweet=None\n",
    "        usernamert=None\n",
    "        for h in data['entities']['hashtags']:\n",
    "            hashtags.append(h['text'])\n",
    "            \n",
    "        for url in urls:\n",
    "            urlsList.append(url['url'])        \n",
    "            \n",
    "    dictRelevantInfo['tweetID'] = idTweet\n",
    "    dictRelevantInfo['text'] = text\n",
    "    dictRelevantInfo['username'] = username\n",
    "    dictRelevantInfo['date'] = date\n",
    "    dictRelevantInfo['hashtags'] = hashtags\n",
    "    dictRelevantInfo['likes'] = likes\n",
    "    dictRelevantInfo['rt_count'] = rt_count\n",
    "    dictRelevantInfo['urlsList'] = urlsList\n",
    "    dictRelevantInfo['isRetweeted'] = isRt\n",
    "    dictRelevantInfo['idRt'] = id_retweet\n",
    "    dictRelevantInfo['usernameRT'] = usernamert\n",
    "    return dictRelevantInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanTweets = {}\n",
    "for t in tweets:\n",
    "    currentTweet=getRelevantInfo(t)\n",
    "    tweetID=currentTweet['tweetID']\n",
    "    cleanTweets[tweetID] = currentTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cleanTweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a graph from the interactions of users that Retweets.\n",
    "We ignore all the tweets owner that are original, because not has interaction with other tweets, and later appear in the graph if other users retweet its tweets.\n",
    "    \n",
    "    Argument:  cleanTweets -- dictionary of dictionaries with all relevant info for each tweet\n",
    "    Returns:   g -- a directed graph with edges between User 1 --> User 2 (where user1 retweet user2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRtGraph(cleanTweets):\n",
    "    freqRT={}\n",
    "    g=nx.DiGraph()\n",
    "    for t in cleanTweets:\n",
    "        ## ONLY GENERATES A GRAPH WITH THE INTERACTIONS ON THE RETWEETS. IF THE CURRENT TWEET IS NOT A RT,\n",
    "        ## WE IGNORE IT\n",
    "        if cleanTweets[t]['isRetweeted']==True and cleanTweets[t][\"idRt\"] in cleanTweets:\n",
    "            key=cleanTweets[t][\"username\"]+\"->\"+cleanTweets[t][\"usernameRT\"]\n",
    "            if key in freqRT:\n",
    "                freqRT[key] += 0.5\n",
    "            else:\n",
    "                freqRT[key] = 0.1\n",
    "            # GENERATE EDGES (AND NODES, add_edge() ADDS NODES IF NOT EXIST), FROM THE RT USER TO ORIGINAL USER.\n",
    "            g.add_edge(cleanTweets[t][\"username\"],cleanTweets[t][\"usernameRT\"],weight=freqRT[key])  \n",
    "    return g, freqRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph, freqRT=getRtGraph(cleanTweets)\n",
    "\n",
    "## WEIGHT EDGES\n",
    "weighted_edges=[]\n",
    "width=[]\n",
    "for (u,v,data) in graph.edges(data=True):\n",
    "    weighted_edges.append((u,v))\n",
    "    width.append(data['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "pos=nx.spring_layout(graph, iterations=5)\n",
    "nx.draw_networkx_nodes(graph, pos, node_size=5, node_color='red') \n",
    "_=nx.draw_networkx_edges(graph,pos,edgelist=weighted_edges,width=width)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(freqRT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph.number_of_nodes())\n",
    "for x in graph.edges():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_shuffle=list(nx.to_edgelist(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rnd.shuffle(edges_shuffle)\n",
    "\n",
    "train=edges_shuffle[:int(len(edges_shuffle)*0.8)]\n",
    "test=edges_shuffle[int(len(edges_shuffle)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(edges_shuffle))\n",
    "print(len(test))\n",
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PAGE RANK\n",
    "pagerank = nx.pagerank(graph)\n",
    "\n",
    "pagerank_sorted=dict(sorted(pagerank.items(), key=lambda item: item[1] , reverse=True))\n",
    "cont=0\n",
    "for x in pagerank_sorted:\n",
    "    try:\n",
    "        print(cleanTweets[x])\n",
    "    except:\n",
    "        cont+=1\n",
    "    print(x)\n",
    "    print(pagerank_sorted[x])\n",
    "    print()\n",
    "\n",
    "print(cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x in nx.non_neighbors(graph,'1334878351970938883'):\n",
    "    print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.number_of_nodes(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g22=graph.to_undirected(reciprocal=False, as_view=False)\n",
    "nodes_bons=[]\n",
    "for g in g22.nodes():\n",
    "    if g22.degree(g)>=1:\n",
    "        nodes_bons.append(g)\n",
    "print(len(nodes_bons))\n",
    "ebunch=[]\n",
    "for edg in nodes_bons:\n",
    "    for edg2 in nodes_bons:\n",
    "        if edg!=edg2:\n",
    "            ebunch.append((edg,edg2))\n",
    "            \n",
    "prediction=nx.adamic_adar_index(g22,ebunch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pr2=dict(sorted(prediction.items(), key=lambda item: item[2] , reverse=True))\n",
    "print(test2[0])\n",
    "print(prediction)\n",
    "for v in prediction:\n",
    "    if v[2]!=0:\n",
    "        print(v)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_item_user = sparse.csr_matrix((data['event'].astype(float), (data['itemid'], data['visitorid'])))\n",
    "\n",
    "model = implicit.als.AlternatingLeastSquares(factors=20, regularization=0.1, iterations=20)alpha_val = 40\n",
    "data_conf = (sparse_item_user * alpha_val).astype('double')model.fit(data_conf)\n",
    "user_id =   14recommended = model.recommend(user_id, sparse_user_item)print(recommended)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
