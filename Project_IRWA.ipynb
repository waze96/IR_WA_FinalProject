{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHK-yP0fTAP9"
   },
   "source": [
    "# Information Retrieval and Web Analytics: Indexing + Modeling (TF-IDF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 617,
     "status": "ok",
     "timestamp": 1603470874764,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -120
    },
    "id": "9NUq8PwCTAQA",
    "outputId": "8fa7a9c5-8127-4cae-acb1-44239837794b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/waze/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# if you do not have nltk the following command should work \"python -m pip install nltk\" \n",
    "import nltk\n",
    "nltk.download('stopwords');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 615,
     "status": "ok",
     "timestamp": 1603473061105,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -120
    },
    "id": "8F9PVJlbTAQF",
    "outputId": "1ea5d092-3739-495f-a112-4b492561eb71"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "data_path = './data/one2.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlNg4TFXTAQI"
   },
   "source": [
    "#### Load data into memory\n",
    "\n",
    "As mentioned above the dataset is stored in a tsv file ```parsed_input_500.tsv```and it contains 500 Wikipedia article (one article per line). For each article we have id, title and body separated by \"|\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2796,
     "status": "ok",
     "timestamp": 1603473064968,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -120
    },
    "id": "X4P3KzRMTAQJ"
   },
   "outputs": [],
   "source": [
    "docs_path = data_path\n",
    "with open(docs_path) as fp:\n",
    "    lines = fp.readlines()\n",
    "tweets = [l.strip().replace(' +', ' ') for l in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 557,
     "status": "ok",
     "timestamp": 1603473068069,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -120
    },
    "id": "BiGNBAAeTAQM",
    "outputId": "4444d85b-c071-44f1-c385-454b4bf5207d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total numer of tweets in the corpus: 1001\n"
     ]
    }
   ],
   "source": [
    "print(\"Total numer of tweets in the corpus: {}\" .format(len(tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweet(tweetText):\n",
    "    \"\"\"\n",
    "    Preprocess the article text (title + body) removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    line -- string (text) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    line - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "        \n",
    "    stemming = PorterStemmer()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    ## START CODE\n",
    "    line= tweetText.lower() ## Transform in lowercase\n",
    "    line=re.sub('[:\\[\\]&%$\\\"\\'!./,;:?=¿^\\-#_*+)<>(¡@]','',line)\n",
    "    line= line.split() ## Tokenize the text to get a list of terms\n",
    "    line= [word for word in line if word not in stops]  ##eliminate the stopwords (HINT: use List Comprehension)\n",
    "    line= [stemming.stem(word) for word in line] ## perform stemming (HINT: use List Comprehension)\n",
    "    ## END CODE\n",
    "    return line\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def getRelevantInfo(tweet):\n",
    "    dictRelevantInfo ={}\n",
    "    data = json.loads(tweet)\n",
    "    hashtags = []\n",
    "    urlsList = []\n",
    "    text = ''\n",
    "    date = data['created_at'] ## ??? RT o no RT\n",
    "    try:\n",
    "        isRt=True\n",
    "        isRetweet=data[\"retweeted_status\"]\n",
    "        idTweet=isRetweet[\"id_str\"]\n",
    "        text = isRetweet['text']\n",
    "        username = isRetweet['user']['screen_name']\n",
    "        urls = isRetweet['entities']['urls']\n",
    "        rt_count = isRetweet['retweet_count']\n",
    "        likes = isRetweet['favorite_count']\n",
    "        \n",
    "        for h in isRetweet['entities']['hashtags']:\n",
    "            hashtags.append(h['text'])\n",
    "        for url in urls:\n",
    "            urlsList.append(url['url'])\n",
    "            \n",
    "    except:\n",
    "        isRt=False\n",
    "        idTweet=data[\"id_str\"]\n",
    "        text = data['text']\n",
    "        username = data['user']['screen_name']\n",
    "        urls = data['entities']['urls']\n",
    "        rt_count=data['retweet_count']\n",
    "        likes = data['favorite_count']\n",
    "        \n",
    "        for h in data['entities']['hashtags']:\n",
    "            hashtags.append(h['text'])\n",
    "            \n",
    "        for url in urls:\n",
    "            urlsList.append(url['url'])        \n",
    "            \n",
    "    dictRelevantInfo['tweetID'] = idTweet\n",
    "    dictRelevantInfo['text'] = text\n",
    "    dictRelevantInfo['tokens'] = cleanTweet(text)\n",
    "    dictRelevantInfo['username'] = username\n",
    "    dictRelevantInfo['date'] = date\n",
    "    dictRelevantInfo['hashtags'] = hashtags\n",
    "    dictRelevantInfo['likes'] = likes\n",
    "    dictRelevantInfo['rt_count'] = rt_count\n",
    "    dictRelevantInfo['urlsList'] = urlsList\n",
    "    dictRelevantInfo['isRetweeted'] = isRt\n",
    "    return dictRelevantInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanTweets = {}\n",
    "for t in tweets:\n",
    "    currentTweet=getRelevantInfo(t)\n",
    "    tweetID=currentTweet['tweetID']\n",
    "    isRtCurrent=currentTweet['isRetweeted']\n",
    "    # Orignial tweet found, overwrite if retweet exist.\n",
    "    if isRtCurrent == False:\n",
    "        cleanTweets[tweetID] = currentTweet\n",
    "    else:\n",
    "        if tweetID in cleanTweets:\n",
    "            continue\n",
    "        else:\n",
    "            cleanTweets[tweetID] = currentTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of cleaned tweets:  709\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of cleaned tweets: \",len(cleanTweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 579,
     "status": "ok",
     "timestamp": 1603473436846,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -120
    },
    "id": "j2uIObKMTAQS"
   },
   "outputs": [],
   "source": [
    "def create_index(lines):\n",
    "    \"\"\"\n",
    "    Impleent the inverted index\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a python dictionary) containing terms as keys and the corresponding \n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    \"\"\"\n",
    "    index=defaultdict(list) \n",
    "    tweetIndex = {} # dictionary to map tweets to page ids\n",
    "    for line in lines.values(): # Remember, lines contain all tweets, each line is a tweet\n",
    "        tweetID = line['tweetID']\n",
    "        terms = line['tokens'] #page_title + page_text\n",
    "        tweetIndex[tweetID]=line['text']  ## we do not need to apply get terms to title because it used only to print titles and not in the index\n",
    "        \n",
    "        ## ===============================================================        \n",
    "        ## create the index for the current doc and store it in termdictPage\n",
    "        ## termdictPage ==> { ‘term1’: [currentdoc, [list of positions]], ...,‘termn’: [currentdoc, [list of positions]]}\n",
    "        \n",
    "        ## Example: if the curr_doc has id 1 and his text is \n",
    "        ## \"web retrieval information retrieval\":\n",
    "        \n",
    "        ## termdictPage ==> { ‘web’: [1, [0]], ‘retrieval’: [1, [1,4]], ‘information’: [1, [2]]}\n",
    "        \n",
    "        ## the term ‘web’ appears in document 1 in positions 0, \n",
    "        ## the term ‘retrieval’ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "        \n",
    "        termdictPage={}\n",
    "\n",
    "        for position, term in enumerate(terms): # terms contains page_title + page_text. Loop over all terms\n",
    "            try:\n",
    "                # if the term is already in the index for the current page (termdictPage)\n",
    "                # append the position to the corrisponding list\n",
    "                \n",
    "        ## START CODE\n",
    "                termdictPage[term][1].append(position)  \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                termdictPage[term]=[tweetID, array('I',[position])] #'I' indicates unsigned int (int in python)\n",
    "            \n",
    "        #merge the current page index with the main index\n",
    "        for termpage, postingpage in termdictPage.items():\n",
    "            index[termpage].append(postingpage)\n",
    "        \n",
    "        ## END CODE                    \n",
    "                    \n",
    "    return index, tweetIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 7462,
     "status": "ok",
     "timestamp": 1603475667598,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -120
    },
    "id": "L388FPECTAQV",
    "outputId": "2669a9b6-d767-4b78-f617-130ba69488c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 0.01 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# index is the dict of , index by twee ?¿?¿\n",
    "# tweetIndex is the dict of tweets, indexed by tweetID\n",
    "index, tweetIndex = create_index(cleanTweets)\n",
    "\n",
    "print(\"Total time to create the index: {} seconds\" .format(np.round(time.time() - start_time,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyd2WyJOTAQY"
   },
   "source": [
    "Notice that if you look in the index for ```researcher```you will not find any result, while if you look for ```research``` you will get some results. That happens because we are storing in the index stemmed terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "executionInfo": {
     "elapsed": 846,
     "status": "ok",
     "timestamp": 1603473583036,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -120
    },
    "id": "mZuV9k_ATAQZ",
    "outputId": "93d2bb77-8ca6-4d9d-acab-b5896f5aad53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Index results for the term 'biden': 8\n",
      "\n",
      "First 10 Index results for the term 'biden': \n",
      "[['1331952640574459905', array('I', [1])], ['1333092424936284163', array('I', [3])], ['1333092425611554823', array('I', [8])], ['1333088877343608832', array('I', [6])], ['1333026911799439361', array('I', [7])], ['1332712274000293889', array('I', [3])], ['1333092426953658368', array('I', [7])], ['1333023981646057473', array('I', [6])], ['1333087495509012480', array('I', [0])], ['1333088498975436804', array('I', [2])]]\n"
     ]
    }
   ],
   "source": [
    "#print(\"Index results for the term 'biden': {}\\n\".format(index['biden']))\n",
    "print(\"Length of Index results for the term 'biden': {}\\n\".format(len(index['obama'])))\n",
    "print(\"First 10 Index results for the term 'biden': \\n{}\".format(index['biden'][:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResultsFromIndex(word, index):\n",
    "    stemming = PorterStemmer()\n",
    "    newWord=word.lower() ## Transform in lowercase\n",
    "    newWord=re.sub('[:\\[\\]&%$\\\"\\'!./,;:?=¿^\\-#_*+)<>(¡@]','',word)\n",
    "    newWord= stemming.stem(word) ## perform stemming\n",
    "    print(\"Length of Index results for the term '{}': {}\".format(newWord, len(index[newWord])))\n",
    "    print(\"First 10 Index results for the term '{}': \\n{}\\n\\n\".format(newWord, index[newWord][:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Index results for the term 'trump': 326\n",
      "First 10 Index results for the term 'trump': \n",
      "[['1332994175281868801', array('I', [6])], ['1333068258862370819', array('I', [0])], ['1333068652665516036', array('I', [3])], ['1332927520270913536', array('I', [8])], ['1333057200802107393', array('I', [9])], ['1333091717579501569', array('I', [2])], ['1333071374081011715', array('I', [5])], ['1333023981646057473', array('I', [3])], ['1333070773758685194', array('I', [0])], ['1332844785334358016', array('I', [1])]]\n",
      "\n",
      "\n",
      "Length of Index results for the term 'biden': 155\n",
      "First 10 Index results for the term 'biden': \n",
      "[['1331952640574459905', array('I', [1])], ['1333092424936284163', array('I', [3])], ['1333092425611554823', array('I', [8])], ['1333088877343608832', array('I', [6])], ['1333026911799439361', array('I', [7])], ['1332712274000293889', array('I', [3])], ['1333092426953658368', array('I', [7])], ['1333023981646057473', array('I', [6])], ['1333087495509012480', array('I', [0])], ['1333088498975436804', array('I', [2])]]\n",
      "\n",
      "\n",
      "Length of Index results for the term 'joe': 50\n",
      "First 10 Index results for the term 'joe': \n",
      "[['1333088877343608832', array('I', [5])], ['1333026911799439361', array('I', [6])], ['1332712274000293889', array('I', [2])], ['1333088498975436804', array('I', [1])], ['1333089405523824644', array('I', [5])], ['1333025108458430464', array('I', [0])], ['1332391784278798338', array('I', [11])], ['1324907501498855425', array('I', [2])], ['1332816778578169861', array('I', [5])], ['1333034437647413250', array('I', [6])]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "getResultsFromIndex(\"trump\", index)\n",
    "getResultsFromIndex(\"biden\", index)\n",
    "getResultsFromIndex(\"joe\", index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atoTwD8uTAQc"
   },
   "source": [
    "### Querying the Index\n",
    "\n",
    "Even if before we mentioned that in case of phrase queries we need to take into account the position of the terms in the document and we have implemented an index that would allow us to also work with this type of queries, here you are going to implement a search function that will query the index without take into account the trems' positions.\n",
    "\n",
    "\n",
    "We will use english Free Text Queries, that means that the query we will query the index using  a sequence of english words as query, and the output will be the list of documents that contain any of the query terms. \n",
    "\n",
    "For instance if we write the query **\"computer science\"** the output will be the union of all documents containing the term \"computer\" with all documents containing the term \"science\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 610,
     "status": "ok",
     "timestamp": 1603474126566,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -120
    },
    "id": "HgJ-tjraTAQc"
   },
   "outputs": [],
   "source": [
    "def search(query, index):\n",
    "    '''\n",
    "    The output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    '''\n",
    "    query=cleanTweet(query)\n",
    "    tweets=set()\n",
    "    for term in query:\n",
    "    ## START DODE\n",
    "        try:\n",
    "            # store in termTweets the ids of the tweets that contain \"term\"                        \n",
    "            termTweets=[posting[0] for posting in index[term]]\n",
    "            # tweets = tweets Union termTweets\n",
    "            tweets = tweets.union(termTweets)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    tweets=list(tweets)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "executionInfo": {
     "elapsed": 9085,
     "status": "ok",
     "timestamp": 1603474152854,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -120
    },
    "id": "TZ9WpXCLTAQf",
    "outputId": "62304f17-8569-491c-f185-a168289814df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:\n",
      "\n",
      "biden\n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 155 for the seached query:\n",
      "\n",
      "tweet_id= 1333051565352038400\n",
      "tweet_text: I'll never call Biden my President, \n",
      "I'll never bow to Socialism &amp; I'll never accept the Radical agenda the left wa… https://t.co/S722x9pzMI\n",
      "\n",
      "\n",
      "\n",
      "tweet_id= 1291460348852264961\n",
      "tweet_text: 🔥Betrayal: SEAL Team 6 May 3 2011, at a national event in DC, VP Biden did the unthinkable - He publicly revealed t… https://t.co/PIfzZBsOTK\n",
      "\n",
      "\n",
      "\n",
      "tweet_id= 1333068370292445188\n",
      "tweet_text: The Pennsylvania supreme Court changed the election laws to benefit Joe Biden and the Democrats! Subscribe for free… https://t.co/DIXwV4awF3\n",
      "\n",
      "\n",
      "\n",
      "tweet_id= 1333090140168839168\n",
      "tweet_text: Biden won again\n",
      "\n",
      "\n",
      "\n",
      "tweet_id= 1333023981646057473\n",
      "tweet_text: HEROES AT EPA HOLDING OFF TRUMP'S REVENGE ORDERS\n",
      "But now, with the Biden administration on the horizon, career E.P.… https://t.co/3BSO8qcZXN\n",
      "\n",
      "\n",
      "\n",
      "tweet_id= 1333092459933540354\n",
      "tweet_text: @lilbratley Biden is being tone deaf. With most people having 60k or more simply from a bachelors, simply forgiving… https://t.co/Z0XrZmjMVE\n",
      "\n",
      "\n",
      "\n",
      "tweet_id= 1333092486177296385\n",
      "tweet_text: China and the bidens first. To hell with America\n",
      "\n",
      "\n",
      "\n",
      "tweet_id= 1333092498479132673\n",
      "tweet_text: Progressives are pushing for a bold Biden agenda. They might have to work around Congress to do it.\n",
      "\n",
      "As control in… https://t.co/Gshc1Nqprd\n",
      "\n",
      "\n",
      "\n",
      "tweet_id= 1333092489754972164\n",
      "tweet_text: @realDonaldTrump Joe Biden beat YOU!!\n",
      "YOU IDIOT!!!\n",
      "\n",
      "\n",
      "\n",
      "tweet_id= 1333075185021366273\n",
      "tweet_text: BREAKING:  Foreign Intel reports Putin stating Biden has been chosen by the American People. He warns privately tha… https://t.co/o5GdOPGX6S\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query:\\n\")\n",
    "query = input()\n",
    "docs = search(query, index)    \n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the seached query:\\n\".format(top, len(docs)))\n",
    "for d_id in docs[:top] :\n",
    "    print(\"tweet_id= {}\\ntweet_text: {}\\n\\n\\n\".format(d_id, tweetIndex[d_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMUIrY9bTAQi"
   },
   "source": [
    "### Ranking tf-idf\n",
    "\n",
    "When searching in a search engine, we are interested in obtain the results sorted by relevance or by some other criteria. Notice that **the above results are not ranked**.\n",
    "\n",
    "Here you are going to implement **tf-idf (Term Frequency — Inverse Document Frequency)** and use it to obtain a list of ordered results.\n",
    "\n",
    "Tf-idf is a weighting scheme that assigns each term in a document a weight based on its term frequency (tf) and inverse document frequency (idf).  The higher the scores, more important the term is. \n",
    "\n",
    "##### TF\n",
    "**tf** refers to the frequency of a term $t$ in a specific document $d$. The basic idea is that as a term appears more in the document it becomes more important. On the other side, if we only use pure term counts, longer documents will be favored more. Consider two documents with exactly the same content but one being twice longer by concatenating with itself.  The tf weights of each word in the longer document will be twice the shorter one, although they essentially have the same content. To deal with this issue we need to **normalize the term frequencies**.\n",
    "\n",
    "$$tf_{t,d} = \\dfrac{N_{t,d}}{||D||}\\tag{1}$$\n",
    "\n",
    "\n",
    "\n",
    "where ||D|| is the Euclidean norm. \n",
    "\n",
    "\n",
    "Let $D=[t_1, t_2, \\dots, t_n]$ be the document vector where $t_i$ represent the frequency of the term $i$, the  Euclidean Norm is calculated as\n",
    "\n",
    "$$\\sqrt{\\sum_{t=1}^{n}t_i{^2}}\\tag{2}$$\n",
    "\n",
    "Note that $||D||$ is the same for all terms of a document.\n",
    "\n",
    "\n",
    "##### IDF\n",
    "A drawback of tf is that it considers all terms equally important. However, less common terms are more discriminative than others. To deal with this issue we introduce **idf (inverse document frequency)** that takes into account the number of documents containing the term.\n",
    "\n",
    "$$idf_t = log\\dfrac{N}{df_t}\\tag{3}$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $N$ is the total number of documents;\n",
    "- $df_t$ is the number of ocuments containing the term $t$.\n",
    "\n",
    "The log operation is applied to avoid that terms that appears in a high number of documents are considered to be too much less important, in this way we are smoothing (dampening) this difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 560,
     "status": "ok",
     "timestamp": 1603477067467,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -120
    },
    "id": "FMjGU0ZbTAQj"
   },
   "outputs": [],
   "source": [
    "def create_index_tfidf(lines, numDocuments):\n",
    "    \"\"\"\n",
    "    Implement the inverted index and compute tf, df and idf\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    numDocuments -- total number of documents\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a python dictionary) containing terms as keys and the corresponding \n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    tf - normalized term frequency for each term in each document\n",
    "    df - number of documents each term appear in\n",
    "    idf - inverse document frequency of each term\n",
    "    \"\"\"\n",
    "        \n",
    "    index=defaultdict(list)\n",
    "    tf=defaultdict(list) #term frequencies of terms in documents (documents in the same order as in the main index)\n",
    "    df=defaultdict(int)         #document frequencies of terms in the corpus\n",
    "    titleIndex=defaultdict(str)\n",
    "    idf=defaultdict(float)\n",
    "    \n",
    "    for line in lines.values(): # Remember, lines contain all tweets, each line is a tweet\n",
    "        #tweetID = line['tweetID']\n",
    "        tweetID = line['tweetID']        \n",
    "        terms = line['tokens']\n",
    "        tweetIndex[tweetID]=line['text']           \n",
    "        \n",
    "        ## ===============================================================        \n",
    "        ## create the index for the **current page** and store it in termdictPage\n",
    "        ## termdictPage ==> { ‘term1’: [currentdoc, [list of positions]], ...,‘termn’: [currentdoc, [list of positions]]}\n",
    "        \n",
    "        ## Example: if the curr_doc has id 1 and his text is \n",
    "        ## \"web retrieval information retrieval\":\n",
    "        \n",
    "        ## termdictPage ==> { ‘web’: [1, [0]], ‘retrieval’: [1, [1,4]], ‘information’: [1, [2]]}\n",
    "        \n",
    "        ## the term ‘web’ appears in document 1 in positions 0, \n",
    "        ## the term ‘retrieval’ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "        termdictPage={}\n",
    "\n",
    "        for position, term in enumerate(terms): ## terms contains page_title + page_text\n",
    "            try:\n",
    "                # if the term is already in the dict append the position to the corrisponding list\n",
    "                termdictPage[term][tweetID].append(position) \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                termdictPage[term]=[tweetID, array('I',[position])] #'I' indicates unsigned int (int in python)\n",
    "        \n",
    "        #normalize term frequencies\n",
    "        # Compute the denominator to normalize term frequencies (formula 2 above)\n",
    "        # norm is the same for all terms of a document.\n",
    "        norm=0\n",
    "        for term, posting in termdictPage.items(): \n",
    "            # posting is a list containing doc_id and the list of positions for current term in current document: \n",
    "            # posting ==> [currentdoc, [list of positions]] \n",
    "            # you can use it to inferr the frequency of current term.\n",
    "            norm+=len(posting[1])**2\n",
    "        norm=math.sqrt(norm)\n",
    "\n",
    "\n",
    "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in termdictPage.items():     \n",
    "            # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "            tf[term].append(np.round(len(posting[1])/norm,4))  ## SEE formula (1) above\n",
    "            #increment the document frequency of current term (number of documents containing the current term)\n",
    "            df[term]= len(termdictPage[term])  # increment df for current term\n",
    "        \n",
    "        #merge the current page index with the main index\n",
    "        for termpage, postingpage in termdictPage.items():\n",
    "            index[termpage].append(postingpage)\n",
    "            \n",
    "        # Compute idf following the formula (3) above. HINT: use np.log\n",
    "        for term in df:\n",
    "            idf[term] = np.round(np.log(float(numDocuments/df[term])),4)\n",
    "            \n",
    "    return index, tf, df, idf, tweetIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 84674,
     "status": "ok",
     "timestamp": 1603477153237,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -120
    },
    "id": "Mvl-m4H-TAQl",
    "outputId": "11c2673f-463b-42e0-9826-9c43a9af576b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 9.58 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "numTweets = len(cleanTweets)\n",
    "index, tf, df, idf, tweetIndex = create_index_tfidf(cleanTweets, numTweets)\n",
    "print(\"Total time to create the index: {} seconds\" .format(np.round(time.time() - start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDictToCSV(d, filename):\n",
    "    w = csv.writer(open(filename, \"w\"))\n",
    "    for key, val in d.items():\n",
    "        w.writerow([key, val])\n",
    "        \n",
    "def loadDictFromCSV(filename):\n",
    "    tmpDict={}\n",
    "    with open(filename,'r') as data: \n",
    "        for line in csv.reader(data): \n",
    "            tmpDict[line[0]]=line[1]\n",
    "    return tmpDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 82645,
     "status": "ok",
     "timestamp": 1603477153238,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -120
    },
    "id": "2pir0XHJTAQp"
   },
   "outputs": [],
   "source": [
    "def rankDocuments(terms, docs, index, idf, tf, titleIndex):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    titleIndex -- mapping between page id and page title\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "        \n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    # The remaing elements would became 0 when multiplied to the queryVector\n",
    "    docVectors=defaultdict(lambda: [0]*len(terms)) # I call docVectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary \n",
    "    queryVector=[0]*len(terms)    \n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms) # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
    "    # HINT: use when computing tf for queryVector\n",
    "    \n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "    \n",
    "    for termIndex, term in enumerate(terms): #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "                    \n",
    "        ## Compute tf*idf(normalize tf as done with documents)\n",
    "        queryVector[termIndex]=query_terms_count[term]/query_norm * idf[term]\n",
    "\n",
    "        # Generate docVectors for matching docs\n",
    "        for docIndex, (doc, postings) in enumerate(index[term]):\n",
    "            # Example of [docIndex, (doc, postings)]\n",
    "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "            \n",
    "            #tf[term][0] will contain the tf of the term \"term\" in the doc 26            \n",
    "            if doc in docs:\n",
    "                docVectors[doc][termIndex]=tf[term][docIndex] * idf[term]  # TODO: check if multiply for idf\n",
    "\n",
    "    # calculate the score of each tweet\n",
    "    # compute the cosine similarity between queyVector and each docVector:\n",
    "    # HINT: you can use the dot product because in case of normalized vectors it corresponds to the cosine siilarity\n",
    "    # see np.dot\n",
    "    \n",
    "    docScores=[ [np.dot(curDocVec, queryVector)/(np.linalg.norm(curDocVec)*np.linalg.norm(queryVector)), doc] for doc, curDocVec in docVectors.items() ]\n",
    "    docScores.sort(reverse=True)\n",
    "    resultDocs=[(x[0], x[1]) for x in docScores]\n",
    "    #print document titles instead if document id's\n",
    "    #resultDocs=[ titleIndex[x] for x in resultDocs ]\n",
    "    if len(resultDocs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)    \n",
    "    #print ('\\n'.join(resultDocs), '\\n')\n",
    "    return resultDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 80771,
     "status": "ok",
     "timestamp": 1603477153238,
     "user": {
      "displayName": "RAMON VALLÉS",
      "photoUrl": "",
      "userId": "05301961646047881826"
     },
     "user_tz": -120
    },
    "id": "CLdEEhT1TAQr"
   },
   "outputs": [],
   "source": [
    "def search_tf_idf(query, index):\n",
    "    '''\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    '''\n",
    "    query=cleanTweet(query)\n",
    "    docs=set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in termDocs the ids of the docs that contain \"term\"                        \n",
    "            termDocs=[posting[0] for posting in index[term]]\n",
    "            \n",
    "            # docs = docs Union termDocs\n",
    "            docs = docs.union(termDocs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs=list(docs)\n",
    "    ranked_docs = rankDocuments(query, docs, index, idf, tf, tweetIndex)   \n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "EpTgX-pTTAQu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:\n",
      "\n",
      "can I see elections on netflix?\n",
      "\n",
      "======================\n",
      "Top 20 results out of 91 for the seached query:\n",
      "\n",
      "Tweet ID= 1333069486484840450\n",
      "Tweet: Establishment GOP is obtuse: Trump voters see election fraud as part of rolling coup targeting POTUS. If vote is re… https://t.co/XdvbVpFhOy\n",
      "Score: 0.8165\n",
      "\n",
      "Tweet ID= 1333026911799439361\n",
      "Tweet: Turn off Netflix, NFL, and Fox News  tell you Joe Biden is President elect!!!  You ask what can we do ?  Read… https://t.co/hB9NwTZyy3\n",
      "Score: 0.8165\n",
      "\n",
      "Tweet ID= 1333092500102377472\n",
      "Tweet: The results of the Presidential election do not fit with historic norms, it's highly doubtful that Biden really won… https://t.co/4YOcoakHcf\n",
      "Score: 0.5774\n",
      "\n",
      "Tweet ID= 1333092493324398598\n",
      "Tweet: @thehill Every news outlet that is not Fox/OANN/Newsmax, every election board in states Biden won, every judge find… https://t.co/hvX0iQysQ5\n",
      "Score: 0.5774\n",
      "\n",
      "Tweet ID= 1333092481081208833\n",
      "Tweet: ‘My mind will not change in 6 months’: Trump vows to use ‘125% of my energy’ on hopeless election challenge | Raw S… https://t.co/ZyA2aN0u88\n",
      "Score: 0.5774\n",
      "\n",
      "Tweet ID= 1333092460860502018\n",
      "Tweet: \"These People Were Vicious - I Saw All Kinds of Illegal Behavior\" - Michigan Election Witness Who Saw 4 AM Illegal… https://t.co/SsrlD3yJXu\n",
      "Score: 0.5774\n",
      "\n",
      "Tweet ID= 1333092452631187458\n",
      "Tweet: @toddstarnes Sure. This election was a referendum on Trump. End of story. I'm sure there are R's that voted for Bid… https://t.co/ReHiSV0KNL\n",
      "Score: 0.5774\n",
      "\n",
      "Tweet ID= 1333090954295689219\n",
      "Tweet: @catturd2 @gatewaypundit Prediction: Our weak government overlords in the RNC/DNC allow the election to be stolen t… https://t.co/sM8y9tc36j\n",
      "Score: 0.5774\n",
      "\n",
      "Tweet ID= 1333088498975436804\n",
      "Tweet: The truth is Joe Biden won the election by millions of votes and Maria Bartiromo @MariaBartiromo knows that. Her “i… https://t.co/EcaJsoulKu\n",
      "Score: 0.5774\n",
      "\n",
      "Tweet ID= 1333087443248173056\n",
      "Tweet: \"These People Were Vicious - I Saw All Kinds of Illegal Behavior\" - Michigan Election Witness Who Saw 4 AM Illegal… https://t.co/7mW1OpOFLu\n",
      "Score: 0.5774\n",
      "\n",
      "Tweet ID= 1333085891892482048\n",
      "Tweet: Yeah, important to distinguish between the theory that the pandemic cost Trump the election (maybe not) vs. that in… https://t.co/9WzU5dIcRQ\n",
      "Score: 0.5774\n",
      "\n",
      "Tweet ID= 1333080763831513089\n",
      "Tweet: UNBELIEVABLE!!\n",
      "This is Pennsylvania’s attorney general.\n",
      "\n",
      "Wow, it’s no wonder the states election cycle was so screw… https://t.co/x4s6vB9Hys\n",
      "Score: 0.5774\n",
      "\n",
      "Tweet ID= 1333074914753060864\n",
      "Tweet: Trump is on Fox News complaining about the “vote dumps” that cost him the election in swing states.\n",
      "\n",
      "Those “vote du… https://t.co/XAEAJVmRKt\n",
      "Score: 0.5774\n",
      "\n",
      "Tweet ID= 1333070943644770308\n",
      "Tweet: \"I'm ashamed that I endorsed him\" -- Trump disses Brian Kemp for not doing more to help him steal the election in G… https://t.co/tIKlwe2zi8\n",
      "Score: 0.5774\n",
      "\n",
      "Tweet ID= 1333065550608691200\n",
      "Tweet: Loeffler is knee deep in the swamp, GA\n",
      "\n",
      "GA must drain the swamp by electing Warnock\n",
      "\n",
      "Just as important, if GA flips… https://t.co/HPNjPCdCPq\n",
      "Score: 0.5774\n",
      "\n",
      "Tweet ID= 1333060349843615745\n",
      "Tweet: American Patriots have now paid the ultimate price with their lives to deliver evidence of election fraud to Presid… https://t.co/DuILRnC5UP\n",
      "Score: 0.5774\n",
      "\n",
      "Tweet ID= 1333057200802107393\n",
      "Tweet: The MSM and the left can continue to ignore the corruption attempting to steal the election\n",
      "\n",
      "But President Trump an… https://t.co/I3X2UXG3cQ\n",
      "Score: 0.5774\n",
      "\n",
      "Tweet ID= 1333052165863190531\n",
      "Tweet: \"These People Were Vicious - I Saw All Kinds of Illegal Behavior\" - Michigan Election Witness Who Saw 4 AM Illegal… https://t.co/CVkMxuW4fq\n",
      "Score: 0.5774\n",
      "\n",
      "Tweet ID= 1333041380013256704\n",
      "Tweet: Donald Trump's sinister plan to overturn the election. If it had worked, the republicans would have been fine with… https://t.co/n98mGcTwzR\n",
      "Score: 0.5774\n",
      "\n",
      "Tweet ID= 1332949729857073152\n",
      "Tweet: Let me be perfectly clear...again. [Joe Biden] {lost} the election.\n",
      "President Trump is President-elect. [Joe] TRIED… https://t.co/a4n1CWNIBW\n",
      "Score: 0.5774\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query:\\n\")\n",
    "query = input()\n",
    "ranked_docs = search_tf_idf(query, index)    \n",
    "top = 20\n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the seached query:\\n\".format(top, len(ranked_docs)))\n",
    "for d_score, d_id in ranked_docs[:top] :\n",
    "    print(\"Tweet ID= {}\\nTweet: {}\\nScore: {}\\n\".format(d_id, tweetIndex[d_id], round(d_score, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Model_Tf_IDF_Index_stud.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
