{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_IRWA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHK-yP0fTAP9"
      },
      "source": [
        "# Information Retrieval and Web Analytics: Indexing + Modeling (TF-IDF) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NUq8PwCTAQA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32b699cf-48e9-4f70-ff71-7554ab02f5bf"
      },
      "source": [
        "# if you do not have nltk the following command should work \"python -m pip install nltk\" \n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords');"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8F9PVJlbTAQF",
        "outputId": "eb1ac8da-4846-4131-ba1b-9a6f90054b8e"
      },
      "source": [
        "import time\n",
        "from collections import defaultdict\n",
        "from array import array\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import math\n",
        "import numpy as np\n",
        "import collections\n",
        "from numpy import linalg as la\n",
        "import json\n",
        "import re\n",
        "import csv\n",
        "import sys\n",
        "\n",
        "data_path = './data/one2.json'\n",
        "\n",
        "# ## Uncoment if using google colab\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "drive.mount('/content/drive') \n",
        "data_path = '/content/drive/My Drive/Information retrieval/IR_WA_FinalProject-master/data/one2.json'\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PDgndzJeurq"
      },
      "source": [
        "## **Preprocessing Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlNg4TFXTAQI"
      },
      "source": [
        "#### Load data into memory\n",
        "\n",
        "The dataset ```dataset.jason``` contains a list of N tweets and its information. This dataset has ben made scrapping for tweets that contain any of the words \"Trump\", \"#Trump\", \"Biden\", \"#Biden\", \"#UsElections2020\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4P3KzRMTAQJ"
      },
      "source": [
        "docs_path = data_path\n",
        "with open(docs_path) as fp:\n",
        "    lines = fp.readlines()\n",
        "tweets = [l.strip().replace(' +', ' ') for l in lines]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiGNBAAeTAQM",
        "outputId": "8a8b592a-4a2c-4169-f903-f7bb42f2ecc0"
      },
      "source": [
        "print(\"Total numer of tweets in the corpus: {}\" .format(len(tweets)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total numer of tweets in the corpus: 1001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A3K2wbqUJ9B"
      },
      "source": [
        "#### Clean Tweets\n",
        "Preprocess the text of a concrete tweet removing non alphabetic characters, stop words, stemming, transforming in lowercase and return the tokens of the text.\n",
        "    \n",
        "    Argument:  tweetText -- string (text) to be preprocessed    \n",
        "    Returns:   cleanText - a list of tokens corresponding to the tweetText after the preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Bw4AjprSJHp"
      },
      "source": [
        "def cleanTweet(tweetText):      \n",
        "    stemming = PorterStemmer()\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "\n",
        "    cleanText = tweetText.lower() ## Transform in lowercase\n",
        "    cleanText = re.sub('[:\\[\\]&%$\\\"\\'!./,;:?=¿^\\-#_*+)<>(¡@]','',cleanText)\n",
        "    cleanText = cleanText.split() ## Tokenize the text to get a list of terms\n",
        "    cleanText = [word for word in cleanText if word not in stops]  ##eliminate the stopwords\n",
        "    cleanText = [stemming.stem(word) for word in cleanText] ## perform stemming\n",
        "    return cleanText\n",
        "    "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryhRZ8N-VTHI"
      },
      "source": [
        "#### Crate a dictionary for each tweet\n",
        "For each tweet, creates a dictionary containing the most relevant information of it (Username, OriginalText, Clean Tokens, number of Likes, number of retweets, list of URLs...)\n",
        "    \n",
        "    Argument:  tweet -- a jason tweet content    \n",
        "    Returns:   dictRelevantInfo -- a dictionary with the processed weet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "_dUAcWTaSJHp"
      },
      "source": [
        "def getRelevantInfo(tweet):\n",
        "    dictRelevantInfo ={}\n",
        "    data = json.loads(tweet)\n",
        "    hashtags = []\n",
        "    urlsList = []\n",
        "    text = ''\n",
        "    date = data['created_at'] ## ??? RT o no RT\n",
        "    try:\n",
        "        isRt=True\n",
        "        isRetweet=data[\"retweeted_status\"]\n",
        "        idTweet=isRetweet[\"id_str\"]\n",
        "        text = isRetweet['text']\n",
        "        username = isRetweet['user']['screen_name']\n",
        "        urls = isRetweet['entities']['urls']\n",
        "        rt_count = isRetweet['retweet_count']\n",
        "        likes = isRetweet['favorite_count']\n",
        "        \n",
        "        for h in isRetweet['entities']['hashtags']:\n",
        "            hashtags.append(h['text'])\n",
        "        for url in urls:\n",
        "            urlsList.append(url['url'])\n",
        "            \n",
        "    except:\n",
        "        isRt=False\n",
        "        idTweet=data[\"id_str\"]\n",
        "        text = data['text']\n",
        "        username = data['user']['screen_name']\n",
        "        urls = data['entities']['urls']\n",
        "        rt_count=data['retweet_count']\n",
        "        likes = data['favorite_count']\n",
        "        \n",
        "        for h in data['entities']['hashtags']:\n",
        "            hashtags.append(h['text'])\n",
        "            \n",
        "        for url in urls:\n",
        "            urlsList.append(url['url'])        \n",
        "            \n",
        "    dictRelevantInfo['tweetID'] = idTweet\n",
        "    dictRelevantInfo['text'] = text\n",
        "    dictRelevantInfo['tokens'] = cleanTweet(text)\n",
        "    dictRelevantInfo['username'] = username\n",
        "    dictRelevantInfo['date'] = date\n",
        "    dictRelevantInfo['hashtags'] = hashtags\n",
        "    dictRelevantInfo['likes'] = likes\n",
        "    dictRelevantInfo['rt_count'] = rt_count\n",
        "    dictRelevantInfo['urlsList'] = urlsList\n",
        "    dictRelevantInfo['isRetweeted'] = isRt\n",
        "    return dictRelevantInfo"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWx9_43aY9jC"
      },
      "source": [
        "#### cleanTweets dict & Drop Duplicates\n",
        "`Here we create a Dictionari (key::TweetID) of tweets. To do so, we iterate over the list of tweets from the dataset, preproces the tweet, and add it to the cleanTweets dictionary if it havent been added before (check for duplicates)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqTtNufmSJHp"
      },
      "source": [
        "cleanTweets = {}\n",
        "for t in tweets:\n",
        "    currentTweet = getRelevantInfo(t)\n",
        "    tweetID = currentTweet['tweetID']\n",
        "    # Orignial tweet found, add to the dict or overwrite if retweet already exist.\n",
        "    if currentTweet['isRetweeted'] == False:\n",
        "        cleanTweets[tweetID] = currentTweet\n",
        "    else:\n",
        "        if tweetID in cleanTweets:\n",
        "            continue\n",
        "        else:\n",
        "            cleanTweets[tweetID] = currentTweet"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewXj_-iqSJHp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2698237f-e7e6-46fc-900f-8e1971dd6219"
      },
      "source": [
        "print(\"Length of cleaned tweets: \",len(cleanTweets))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of cleaned tweets:  709\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCscsGdCMRX6"
      },
      "source": [
        "## **Building the Search Engine**\n",
        "    Argument:    terms -- list of query terms\n",
        "                 docs -- list of documents, to rank, matching the query\n",
        "                 index -- inverted index data structure\n",
        "                 idf -- inverted document frequencies\n",
        "                 tf -- term frequencies\n",
        "                 titleIndex -- mapping between tweet id and tweet content\n",
        "    \n",
        "    Returns:     resultDocs -- list of tweetIDs in decreasing order and its score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biD9sERPENAQ"
      },
      "source": [
        "### Creating tf-idf dictionary\n",
        "\n",
        "Implement the inverted index and compute tf, df and idf\n",
        "\n",
        "\n",
        "\n",
        "    Argument:   cleanTweets -- collection of tweets\n",
        "                numTweets -- total number of tweets\n",
        "    \n",
        "    Returns:    index - the inverted index (implemented through a python dictionary) containing terms as keys and the corresponding \n",
        "                list of document these keys appears in (and the positions) as values.\n",
        "                tf - normalized term frequency for each term in each document\n",
        "                df - number of documents each term appear in\n",
        "                idf - inverse document frequency of each term"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMjGU0ZbTAQj"
      },
      "source": [
        "def create_index_tfidf(cleanTweets, numTweets):\n",
        "    # lines -> cleanTweets\n",
        "    # numDocs -> numOfTweets\n",
        "        \n",
        "    index=defaultdict(list)\n",
        "    tf=defaultdict(list)        #term frequencies of terms in tweets\n",
        "    df=defaultdict(int)         #document frequencies of terms in the corpus\n",
        "    titleIndex=defaultdict(str)\n",
        "    idf=defaultdict(float)\n",
        "\n",
        "    tweetIndex=defaultdict(float)\n",
        "    \n",
        "    for line in cleanTweets.values(): # Remember, cleanTweets contain all tweets, each line is a tweet\n",
        "        tweetID = line['tweetID']        \n",
        "        terms = line['tokens']\n",
        "        tweetIndex[tweetID]=line['text'] \n",
        "\n",
        "        termdictPage={}\n",
        "        for position, term in enumerate(terms): ## terms contains all the tokens of the actual tweet\n",
        "            try:\n",
        "                # if the term is already in the dict append the position to the corrisponding list\n",
        "                termdictPage[term][tweetID].append(position) \n",
        "            except:\n",
        "                # Add the new term as dict key and initialize the array of positions and add the position\n",
        "                termdictPage[term]=[tweetID, array('I',[position])] #'I' indicates unsigned int (int in python)\n",
        "        \n",
        "        # normalize term frequencies\n",
        "        # Compute the denominator to normalize term frequencies\n",
        "        # norm is the same for all terms of a document.\n",
        "        norm=0\n",
        "        for term, posting in termdictPage.items(): \n",
        "            # posting is a list containing tweetID and the list of positions for current term in current tweet: \n",
        "            # posting ==> [tweetID, [list of positions]] \n",
        "            norm+=len(posting[1])**2\n",
        "        norm=math.sqrt(norm)\n",
        "\n",
        "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
        "        for term, posting in termdictPage.items():     \n",
        "            # append the tf for current term (tf = term frequency in current tweet/norm)\n",
        "            tf[term].append(np.round(len(posting[1])/norm,4))\n",
        "            #increment the tweet frequency of current term (number of tweets containing the current term)\n",
        "            df[term]= len(termdictPage[term])  # increment df for current term\n",
        "        \n",
        "        #merge the current page index with the main index\n",
        "        for termpage, postingpage in termdictPage.items():\n",
        "            index[termpage].append(postingpage)\n",
        "            \n",
        "        # Compute idf\n",
        "        for term in df:\n",
        "            idf[term] = np.round(np.log(float(numTweets/df[term])),4)\n",
        "            \n",
        "    return index, tf, df, idf, tweetIndex\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mvl-m4H-TAQl",
        "outputId": "f25c9d8e-b4cd-4ba4-e337-66f2b8da716e"
      },
      "source": [
        "start_time = time.time()\n",
        "numTweets = len(cleanTweets)\n",
        "index, tf, df, idf, tweetIndex = create_index_tfidf(cleanTweets, numTweets)\n",
        "print(\"Total time to create the index: {} seconds\" .format(np.round(time.time() - start_time,2)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total time to create the index: 11.49 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P__OBmrNEaui"
      },
      "source": [
        "### Ranking tweets based on TF-IDFs + Cosine Similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pir0XHJTAQp"
      },
      "source": [
        "def rankDocuments_TFIDF(terms, docs, index, idf, tf, titleIndex):\n",
        "            \n",
        "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
        "    # The remaing elements would became 0 when multiplied to the queryVector\n",
        "    docVectors=defaultdict(lambda: [0]*len(terms)) # I call docVectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary \n",
        "    queryVector=[0]*len(terms)    \n",
        "\n",
        "    # compute the norm for the query tf\n",
        "    query_terms_count = collections.Counter(terms) # get the frequency of each term in the query. \n",
        "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
        "    \n",
        "    query_norm = la.norm(list(query_terms_count.values()))\n",
        "    \n",
        "    for termIndex, term in enumerate(terms): #termIndex is the index of the term in the query\n",
        "        if term not in index:\n",
        "            continue\n",
        "                    \n",
        "        ## Compute tf*idf(normalize tf as done with documents)\n",
        "        queryVector[termIndex]=query_terms_count[term]/query_norm * idf[term]\n",
        "\n",
        "        # Generate docVectors for matching docs\n",
        "        for docIndex, (doc, postings) in enumerate(index[term]):\n",
        "            if doc in docs:\n",
        "                docVectors[doc][termIndex]=tf[term][docIndex] * idf[term]\n",
        "\n",
        "    # calculate the score of each tweet\n",
        "    # compute the cosine similarity between queyVector and each docVector:    \n",
        "    docScores=[ [np.dot(curDocVec, queryVector)/(np.linalg.norm(curDocVec)*np.linalg.norm(queryVector)), doc] for doc, curDocVec in docVectors.items() ]\n",
        "    docScores.sort(reverse=True)\n",
        "    resultDocs=[(x[0], x[1]) for x in docScores]\n",
        "    if len(resultDocs) == 0:\n",
        "        print(\"No results found, try again\")\n",
        "        query = input()\n",
        "        docs = search_tf_idf(query, index)  \n",
        "    return resultDocs"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23_nPV1RLBMM"
      },
      "source": [
        "### Ranking tweets based on TF-IDFs + Likes + Retweets + Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqsBn4ykLDX3"
      },
      "source": [
        "def createLikeRTIndex(cleanTweets):\n",
        "    # maxLikes = 0\n",
        "    # maxRT = 1\n",
        "    # for tweet in lines:\n",
        "    #     if lines[tweet]['likes'] > maxLikes:\n",
        "    #         maxLikes = lines[tweet]['likes']\n",
        "    #     if lines[tweet]['rt_count'] > maxRT:\n",
        "    #         maxRT = lines[tweet]['rt_count']\n",
        "    for tweet in cleanTweets:\n",
        "        cleanTweets[tweet]['likes_score'] = (-np.exp(-(cleanTweets[tweet]['likes']/50000))+1)\n",
        "        cleanTweets[tweet]['rt_score'] = (-np.exp(-(cleanTweets[tweet]['rt_count']/25000))+1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6qZYDa4LK22",
        "outputId": "4d20f1f7-8a00-48a1-e7ed-db47ebf4d5af"
      },
      "source": [
        "createLikeRTIndex(cleanTweets)\n",
        "\n",
        "print(cleanTweets['1323630540977852416']['likes'])\n",
        "print(cleanTweets['1323630540977852416']['likes_score'])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "57724\n",
            "0.6847785905742336\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_EfX8hJLLnz"
      },
      "source": [
        "def rankDocuments_Likes_Retweets(terms, docs, index, idf, tf, titleIndex):\n",
        "            \n",
        "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
        "    # The remaing elements would became 0 when multiplied to the queryVector\n",
        "    docVectors=defaultdict(lambda: [0]*len(terms)) # I call docVectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary \n",
        "    queryVector=[0]*len(terms)    \n",
        "\n",
        "    # compute the norm for the query tf\n",
        "    query_terms_count = collections.Counter(terms) # get the frequency of each term in the query. \n",
        "    \n",
        "    query_norm = la.norm(list(query_terms_count.values()))\n",
        "    \n",
        "    for termIndex, term in enumerate(terms): #termIndex is the index of the term in the query\n",
        "        if term not in index:\n",
        "            continue\n",
        "                    \n",
        "        ## Compute tf*idf(normalize tf as done with documents)\n",
        "        queryVector[termIndex]=query_terms_count[term]/query_norm * idf[term]\n",
        "\n",
        "        # Generate docVectors for matching docs\n",
        "        for docIndex, (doc, postings) in enumerate(index[term]):\n",
        "            \n",
        "            if doc in docs:\n",
        "                docVectors[doc][termIndex]=tf[term][docIndex] * idf[term]\n",
        "\n",
        "    # calculate the score of each tweet\n",
        "    # compute the cosine similarity between queyVector and each docVector:\n",
        "       \n",
        "    docScores=[ [(np.dot(curDocVec, queryVector)/(np.linalg.norm(curDocVec)*np.linalg.norm(queryVector)))*0.6+cleanTweets[doc]['likes_score']*0.2+cleanTweets[doc]['rt_score']*0.2, doc] for doc, curDocVec in docVectors.items() ]\n",
        "    docScores.sort(reverse=True)\n",
        "    resultDocs=[(x[0], x[1]) for x in docScores]\n",
        "    #print document titles instead if document id's\n",
        "    #resultDocs=[ titleIndex[x] for x in resultDocs ]\n",
        "    if len(resultDocs) == 0:\n",
        "        print(\"No results found, try again\")\n",
        "        query = input()\n",
        "        docs = search_tf_idf(query, index)    \n",
        "    #print ('\\n'.join(resultDocs), '\\n')\n",
        "    return resultDocs"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrUqrRoIIT1a"
      },
      "source": [
        "## **Search**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLdEEhT1TAQr"
      },
      "source": [
        "def search_tf_idf(query, index, ranking_type = '0'):\n",
        "    '''\n",
        "    output is the list of documents that contain any of the query terms. \n",
        "    So, we will get the list of documents for each query term, and take the union of them.\n",
        "    '''\n",
        "    query=cleanTweet(query)\n",
        "    docs=set()\n",
        "    for term in query:\n",
        "        try:\n",
        "            # store in termDocs the ids of the docs that contain \"term\"                        \n",
        "            termDocs=[posting[0] for posting in index[term]]\n",
        "            \n",
        "            # docs = docs Union termDocs\n",
        "            docs = docs.union(termDocs)\n",
        "        except:\n",
        "            #term is not in index\n",
        "            pass\n",
        "    docs=list(docs)\n",
        "    if ranking_type == '0': # TF-IDF\n",
        "        ranked_docs = rankDocuments_TFIDF(query, docs, index, idf, tf, tweetIndex)   \n",
        "    elif ranking_type == '1': # TF-IDF + Likes + Retweets\n",
        "        ranked_docs = rankDocuments_Likes_Retweets(query, docs, index, idf, tf, tweetIndex)\n",
        "    return ranked_docs"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpTgX-pTTAQu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda20fe6-744f-4018-e421-9b507acbdc48"
      },
      "source": [
        "print(\"Insert your query:\\n\")\n",
        "query = input()\n",
        "ranked_docs = search_tf_idf(query, index, ranking_type = '1')    \n",
        "top = 10\n",
        "\n",
        "print(\"\\n======================\\nTop {} results out of {} for the seached query:\\n\".format(top, len(ranked_docs)))\n",
        "for d_score, d_id in ranked_docs[:top] :\n",
        "    print(\"Tweet ID= {}\\nTweet: {}\\nScore: {}\\n\".format(d_id, tweetIndex[d_id], round(d_score, 4)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Insert your query:\n",
            "\n",
            "Trump Biden\n",
            "\n",
            "======================\n",
            "Top 10 results out of 457 for the seached query:\n",
            "\n",
            "Tweet ID= 1332527601798275073\n",
            "Tweet: Trump really blew $3,000,000 to give Biden 132 more votes, that has to be the L of the decade.\n",
            "Score: 0.9075\n",
            "\n",
            "Tweet ID= 1332407714304110597\n",
            "Tweet: Biden did poorly in big cities (Politico), except those of Detroit (more votes than people!), Philadelphia, Atlanta… https://t.co/0cCyv36E31\n",
            "Score: 0.8023\n",
            "\n",
            "Tweet ID= 1332155514386599937\n",
            "Tweet: A must read. Impossible for Biden to have overcome these, and even greater, odds! https://t.co/cmYFY0va6p\n",
            "Score: 0.7962\n",
            "\n",
            "Tweet ID= 1331674193792929792\n",
            "Tweet: We should gaslight Trump and say he was never president\n",
            "Score: 0.781\n",
            "\n",
            "Tweet ID= 930065838387863552\n",
            "Tweet: It's time to talk about former Vice President Joe Biden, the open sexual predator. A thread/moment...\n",
            "Score: 0.7432\n",
            "\n",
            "Tweet ID= 1332754929778368526\n",
            "Tweet: Alan Dershowitz: Trump was ‘Absolutely Right‘ to Pardon Michael Flynn https://t.co/nMGOgOM9PP via @BreitbartNews\n",
            "Score: 0.7391\n",
            "\n",
            "Tweet ID= 1332719082534293508\n",
            "Tweet: Four Videos - Four States Where Votes Were Switched Live on TV Away from President Trump to Biden via @gatewaypundit https://t.co/HL1UzBrGQN\n",
            "Score: 0.6764\n",
            "\n",
            "Tweet ID= 1323630540977852416\n",
            "Tweet: Don't be surprised if you hear a network call the election for Biden before the polls are closed today.  \n",
            "\n",
            "The disi… https://t.co/Lr2lY3hXQY\n",
            "Score: 0.6705\n",
            "\n",
            "Tweet ID= 1332680233720438785\n",
            "Tweet: \"I'm the president of the United States. Don't ever talk to the president that way.\"\n",
            " -- Donald J. Trump\n",
            "\"No Title… https://t.co/CRjCYOS07C\n",
            "Score: 0.6533\n",
            "\n",
            "Tweet ID= 1332734431107354629\n",
            "Tweet: Biden “won” in the same way that Lance Armstrong “won.” Yes, Armstrong crossed the finish line first. But his victo… https://t.co/HpKu6a5d2Z\n",
            "Score: 0.6515\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}